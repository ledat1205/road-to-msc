![[Screenshot 2025-12-07 at 21.30.53.png]]


![[Screenshot 2025-12-07 at 21.29.45.png]]


![[Screenshot 2025-12-07 at 21.21.31.png]]
![[Screenshot 2025-12-07 at 21.22.24.png]]

- **LSM Trees**: RocksDB, LevelDB, Cassandra
    
- **Append-Only MVCC Storage**: PostgreSQL heap tables

- **Oracle UNDO + Redo**
    
- **ClickHouse parts**
    
- **Kafka logs**

# üîß What Is a Memtable?

A **memtable** is an **in-memory sorted data structure**, usually:

- **Skiplist** (LevelDB, RocksDB default)
    
- **Balanced tree** (Cassandra uses a B+Tree-like structure)
    
- **Red-black tree**
    

It stores the **latest written key-value pairs**.

### Why it exists?

Disk writes are slow ‚ö†Ô∏è  
Random writes are _very_ slow ‚ö†Ô∏è

Memtable keeps new writes **in RAM**, which is:

- Fast
    
- Lock-free in many implementations
    
- Sorted automatically
    

Every write is appended to the **WAL (Write-Ahead Log)** for durability, then inserted into the memtable.

---

# üî• What Happens on INSERT or UPDATE?

### 1Ô∏è‚É£ Client writes a key-value:

`PUT(user123, "Alice")`

### 2Ô∏è‚É£ Database appends to the WAL (sequential disk write)

This guarantees durability.

### 3Ô∏è‚É£ Then updates the Memtable

Memtable stores:

`user123 ‚Üí "Alice" (plus sequence number, tombstone flag, etc.)`

### 4Ô∏è‚É£ Memtable keeps it sorted

So range reads and flush to SST files are easy.

---

# üßä When Does the Memtable Flush?

The memtable is kept in RAM until:

- It becomes **full** (memory threshold reached, e.g., 64MB)
    
- Or system pressure
    
- Or manual flush command
    

Example threshold:

`memtable_size = 64 MB`

Once memtable reaches ~64MB‚Ä¶

üëâ It gets **frozen** and becomes an **immutable memtable**.  
üëâ A new active memtable is created.

---

# üöÄ How Flushing From Memtable ‚Üí SSTable Happens

### Step-by-step flushing process

---

## **1Ô∏è‚É£ Memtable becomes immutable**

Current memtable:

- No longer accepts writes
    
- Becomes a _read-only_ snapshot of memory
    

A new memtable is created to take new writes.

---

## **2Ô∏è‚É£ Background flush thread starts**

A background I/O thread takes the **immutable memtable**.

---

## **3Ô∏è‚É£ The memtable is written to disk as an SSTable**

### SST (Sorted String Table) format:

- sorted keys
    
- values
    
- index blocks
    
- bloom filter
    
- footer
    

The memtable is **already sorted**, so writing is just a **sequential scan + write** (very fast!).

Example SSTable:

`[user001 ‚Üí Alice] [user002 ‚Üí Bob] [user003 ‚Üí Charlie] ...`


# üç± 1. What an SSTable _Really Is_

**SSTable = Sorted String Table**

- Immutable file
    
- Sorted by key
    
- Contains **data blocks**, **index block**, **filter block**, **footer**
    
- Designed for _sequential read_ and _fast lookup_
    

Think of it as:

[SSTable file]
  ‚îú‚îÄ‚îÄ Data Blocks (sorted key-value chunks)
  ‚îú‚îÄ‚îÄ Index Block (points to each data block)
  ‚îú‚îÄ‚îÄ Filter Block (Bloom Filter)
  ‚îú‚îÄ‚îÄ Metaindex Block
  ‚îî‚îÄ‚îÄ Footer (fixed size, last 48 bytes)


---

# üìÅ 2. What EXACTLY is inside an SST file (byte-level view)

Here's an actual **internal structure** like RocksDB/LevelDB:

+--------------------------------------------------------------+
| Data Block #1 (K/V entries)                                  |
+--------------------------------------------------------------+
| Data Block #2 (K/V entries)                                  |
+--------------------------------------------------------------+
| Data Block #3 (K/V entries)                                  |
+--------------------------------------------------------------+
| ...                                                          |
+--------------------------------------------------------------+
| Filter Block (Bloom filters for blocks)                      |
+--------------------------------------------------------------+
| Metaindex Block (pointers to filter block, etc.)             |
+--------------------------------------------------------------+
| Index Block (key ‚Üí block handle mappings)                    |
+--------------------------------------------------------------+
| Footer (16-byte magic + block handles + meta offsets)        |
+--------------------------------------------------------------+

SST files are typically 1‚Äì64 MB in size.

---

# üîç 3. Data Block (the most important part)

Each block stores **many sorted K/V pairs**:

[Data Block]
+------------------------------+
| Restart Point #1             |  \
| Restart Point #2             |   > prefix compression
| Restart Point #3             |  /
+------------------------------+
| Key (partial) | Value        |
| Key (partial) | Value        |
| Key (partial) | Value        |
+------------------------------+
| Restart Count                 |
+------------------------------+

### Why ‚Äúpartial key‚Äù?

To reduce space:

- Store full key at restart points
    
- Store delta/prefix key between restart points
    

Example keys:

`user:0001 user:0002 user:0003 user:0004`

Storage:

Restart #1:  user:0001
Entry 2:     suffix("2")
Entry 3:     suffix("3")
Entry 4:     suffix("4")

---

# üéØ 4. Index Block

The index block maps each data block to its **key range**.

Example:

|Block|Smallest Key|Offset on Disk|
|---|---|---|
|1|user:0001|0x0000FA|
|2|user:0400|0x001234|
|3|user:0800|0x0028A0|

This lets the DB jump directly to the correct part of the SST.

---

# üå∏ 5. Filter Block (Bloom Filters)

Avoids unnecessary disk reads.

Contains Bloom filters like:

`filter for [user:0000 ‚Äì user:03FF]
`filter for [user:0400 ‚Äì user:07FF]` 
`filter for [user:0800 ‚Äì user:0BFF]`

Query:

`GET user:0111    ‚Üí check Bloom filter    ‚Üí if filter says ‚ÄúNO‚Äù ‚Üí skip data block`

Huge performance boost.

---

# üß© 6. Metaindex Block

Contains metadata like:

- pointer to filter block
    
- pointer to compression type
    
- version info
    

Simple key-value pairs.

---

# üß± 7. Footer (last 48 bytes)

Looks like:

[Index Block Offset | Index Block Size]        (16 bytes)
[MetaIndex Offset   | MetaIndex Size]          (16 bytes)
[Magic Number: SST signature]                  (8 bytes)


Magic number example:

`0x88 F2 3A E1 6B BD E2 41`

Footer lets DB open SST files backwards (from the end).

---

# üóÇ 8. How L0, L1, L2‚Ä¶ Look in an LSM Tree

![[Screenshot 2025-12-07 at 22.52.40.png]]

**Level 0 (L0):**

- Files come directly from memtable flush
    
- Overlapping key ranges
    
- Many small SSTs
    
- Searching L0 means scanning multiple SST files
    

Example:

L0:
    file_001.sst  [A ‚Äî F]
    file_002.sst  [C ‚Äî M]
    file_003.sst  [A ‚Äî Z]


Overlaps allowed.

---

**Level 1 (L1):**

- Non-overlapping key ranges
    
- Larger files (10‚Äì50 MB each)
    
- Results from compaction of L0
    

Example:

L1:
    file_010.sst  [A ‚Äî D]
    file_011.sst  [E ‚Äî H]
    file_012.sst  [I ‚Äî M]
    file_013.sst  [N ‚Äî Z]


Only **one file** per key range.

---

**Level 2+:**

- Even larger files
    
- No overlap within each level
    
- Compaction moves data downward
    

Example:

L2:
  file_100.sst   [A ‚Äî H]
  file_101.sst   [I ‚Äî Z]

L3:
  file_200.sst   [A ‚Äî Z]  (hundreds of MB)


LSM Tree looks like:

L0:  many small overlapping files
L1:  fewer non-overlapping files
L2:  even fewer files, bigger
L3:  huge files


---

# üß† 9. Retrieval Path (How a GET Works)

Let‚Äôs say the user calls:

`value = db->Get("user:12345");`

We will walk through **every stage** of the read path.

---

## ü•á Step 1 ‚Äî Query the Memtable (RAM)

üî• Always checked first because:

- It has **newest writes**
    
- It‚Äôs in **RAM** ‚Üí fastest access
    

Memtable is usually:

- Skiplist (LevelDB)
    
- Red-black tree (some implementations)
    
- B-tree or ART (modern systems)
    

### What happens?

1. Search the memtable like a sorted map.
    
2. If key exists:
    
    - Return the associated value (or tombstone)
        
    - STOP ‚Üí done.
        

If not found:

- Check **immutable memtables**.
    

---

## Step 2 ‚Äî Immutable Memtables (Not Yet Flushed)

When a memtable is full, it becomes **immutable** and a background thread flushes it to L0 SSTables.

Immutable tables:

- Are sorted
    
- Are searched the same way as the active memtable
    

If key is found:

- Return value
    
- STOP ‚Üí done
    

If not found:

- Proceed to L0.
    

---

## Step 3 ‚Äî Search L0 SSTables

### WHY is L0 special?

- Many small files (flushes)
    
- **Key ranges overlap**
    
- So multiple files _may_ contain the same key
    

### ORDER:

üîΩ **Newest ‚Üí Oldest**

Because newer files contain higher **sequence numbers**, meaning newer versions.

### For each L0 file:

1. **Check Bloom filter**
    
    - If Bloom filter says "not present": skip file completely (very fast)
        
2. **If Bloom filter might match ‚Üí binary-search the Index Block**
    
3. Find the correct Data Block
    
4. Lookup key inside the Data Block
    
5. If key found:
    
    - Check sequence numbers
        
    - Return the newest visible version
        
    - STOP ‚Üí done
        

If not found in any L0 file:

- Move to L1.
    

---

## Step 4 ‚Äî Search Level 1 SSTables

### KEY PROPERTY OF L1 AND BELOW:

üìå **Non-overlapping key ranges**

This gives a huge optimization:

üëâ There is **exactly one SST file at L1** that _can possibly_ contain this key.

### Process:

1. Binary-search the file metadata for key ranges
    
2. Jump to the exact SST file
    
3. Perform:
    
    - Bloom filter ‚Üí skip or continue
        
    - Index Block lookup
        
    - Data Block lookup
        

If found:

- Return value
    
- STOP ‚Üí done
    

If not found:

- Move to L2.
    

---

## Step 5 ‚Äî Search Levels L2, L3, ‚Ä¶ Ln

Each lower level:

- Has **bigger files**
    
- Ranges do not overlap inside the level
    
- So only **1 file per level** needs to be checked
    

(Sometimes 0 files if key‚Äôs range doesn‚Äôt exist.)

For each level:

1. Binary-search file list
    
2. Check Bloom filter
    
3. Check index ‚Üí block ‚Üí record
    

If found:

- Return value
    
- DONE
    

If not found:

- Continue downward
![[Screenshot 2025-12-10 at 22.12.51.png]]

![[Screenshot 2025-12-10 at 22.12.20.png]]

# Tuple Layout & Tuple Header 

## 1. What Is a Tuple?

A **tuple** = one **row** stored inside a **page/block** in a database.  
Example: a table row

`(id=10, name="Alice", age=25)`

In storage, this becomes:

`[TUPLE HEADER][NULL BITMAP][VARIABLE-LENGTH AREA][FIXED-LENGTH AREA]`

---


# 1. Page Layout (where tuples live)

A typical page (e.g., PostgreSQL 8KB, Oracle 16KB) contains:

+-------------------------+
| Page Header             |
+-------------------------+
| Item/Line Pointers      |
+-------------------------+
| Tuples (physical data)  |
+-------------------------+
| Free Space              |
+-------------------------+


The page header tracks:

- page LSN / checksum
    
- number of tuples
    
- free space offset
    
- MVCC metadata
    

Item pointers:

- point to each tuple‚Äôs **offset** inside the page
    
- allow tuples to move without breaking references
    

---

# 2. Tuple Layout (Row Layout)

A tuple contains:

+------------------------------+
| Tuple Header                 |
+------------------------------+
| NULL Bitmap (optional)       |
+------------------------------+
| Variable-length data         |
+------------------------------+
| Fixed-length data            |
+------------------------------+
| Padding (alignment)          |
+------------------------------+

---

# 3. Tuple Header ‚Äî What Is Inside?

A **tuple header** stores metadata so the DB can manage MVCC, indexing, nulls, and versioning.  
PostgreSQL example:

TupleHeader {
    xmin      // creating transaction ID
    xmax      // deleting or updating txn ID
    xvac      // vacuum ID (old)
    ctid      // tuple identifier (version chaining)
    infomask  // flags: nulls? varlen? keys?
    natts     // number of attributes
    ... other MVCC/control bits
}


### What Tuple Header Encodes

|Field|Meaning|
|---|---|
|**xmin**|Transaction that created this version|
|**xmax**|Transaction that deleted/updated it|
|**ctid**|If updated ‚Üí points to new tuple version (version chain)|
|**NULL bitmap**|Bitmask indicating which columns are NULL|
|**VARLENA flags**|Flags for variable-length types|
|**Visibility flags**|Helps MVCC determine visible versions|

---

# 4. Why Is Tuple Header Needed?

It enables **MVCC** and prevents blocking:

- By checking `(xmin, xmax)` against the snapshot, DB knows **which version** of the row is visible.
    
- When updating, DB **creates a new tuple**, links via `ctid`, and marks old version‚Äôs `xmax`.
    
- Without tuple header, DB cannot provide snapshot isolation.

## Version chain
When a row is updated, the DB forms a **version chain**:

`[Tuple V1] -> [Tuple V2] -> [Tuple V3] -> ...`

This is literally a **linked list** stored inside the page.

### Example:

Before update:

`ctid = (block 12, offset 5)   // points to itself`

After update:

`V1.ctid = (12,5) ‚Üí (12,9)   // new version V2.ctid = (12,9) ‚Üí itself`

Readers follow the chain until they find a version **visible** at their snapshot SCN.


## How MVCC Chooses Which Version You See
### Transaction T1 begins with snapshot SCN = 100

Versions available:

`V1: xmin = 50,  xmax = 120 V2: xmin = 120, xmax = 0`

Interpretation:

- V1 created earlier, deleted by txn 120
    
- V2 created by txn 120
    

At SCN = 100:

- `120 > 100` ‚Üí T1 cannot see V2 (created in the future)
    
- `xmax of V1 = 120 > 100` ‚Üí delete is in the future ‚Üí V1 is still valid
    

So T1 sees **Version 1**.


# 5. Tuple Storage Example

### Schema

CREATE TABLE users (
    id INT,
    name TEXT,
    age INT
);

### Tuple Stored

[Header]
    xmin = 100
    xmax = 0
    ctid = (0,10)
    null_bitmap = 0b000  (no nulls)

[Varlen Area]
    "Alice"

[Fixed Area]
    id = 10
    age = 25


---

# 6. Update Example With Full MVCC Logic

## Step 1 ‚Äî Original row

`Row V1: xmin = 50 xmax = 0 ctid = self`

## Step 2 ‚Äî T2 updates row at SCN = 120

DB:

1. Copies V1 into UNDO (Oracle) or keeps V1 inline (Postgres)
    
2. Creates new version V2
    
3. Sets V1.xmax = 120
    
4. Sets V1.ctid ‚Üí V2
    

`V1:     xmin = 50     xmax = 120     ctid = (block,offset_of_V2) V2:     xmin = 120     xmax = 0     ctid = self`

### Visibility rules:

- Readers with **snapshot < 120** ‚Üí see V1
    
- Readers with **snapshot ‚â• 120** ‚Üí see V2
    

That is MVCC.

---

# 7. Delete Example

DELETE marks the row as ‚Äúnot visible anymore‚Äù by setting:

`xmax = deleting_txn`

But the row isn‚Äôt removed immediately (to support old snapshots).


## 8. NULL Bitmap

If any columns can be NULL, DB adds a **bitmap** with 1 bit per column:

- `1 = NULL`
    
- `0 = NOT NULL`
    

Bitmap is needed because storing actual `NULL` values wastes space.

### Example

table:

`CREATE TABLE users(     id INT,     name TEXT,     age INT,     city TEXT  -- can be null );`

row:

`(1, 'Tom', 25, NULL)`

NULL bitmap (4 columns ‚Üí 1 byte enough):

`Columns:  id  name age city Bitmap:   0    0    0    1`

Meaning: `city` is NULL.

Bitmap goes immediately after header.

---
![[Screenshot 2025-12-10 at 22.32.28.png]]

## 9. Variable-length Attributes (varlen)

Types like:

- `TEXT`, `VARCHAR`, `BYTEA`
    
- arrays
    
- JSON
    
- XML
    
- large objects
    

All go in the **varlen section**.

### Example (PostgreSQL)

Tuple:

`('Tom', 25)`

storage:

`varlen:    name = 'Tom'       length prefix (1 byte or 4 bytes)      data bytes ('T','o','m') fixed:    age = INT: 25`

Varlen fields store _length + data_.

If very large ‚Üí stored outside (TOAST), and tuple stores only a pointer.
### üü¶ How PostgreSQL Compresses Columns (TOAST compression)

**When does compression happen?**

PostgreSQL **automatically** tries to compress a column when:

- The row does **not** fit inside an 8 KB page, **AND**
    
- The column‚Äôs storage mode allows compression  
    (default for most variable-length types)
    

Compression order:

1. üîπ Compress large varlena values
    
2. üîπ If still too large ‚Üí move to TOAST table
    
3. üîπ If still too large ‚Üí split into TOAST chunks
    

So compression happens **before** sending data to TOAST.

---

**What compression algorithm is used?**

PostgreSQL uses:

### ‚úî **pglz (PostgreSQL LZ variant)**

- A custom LZ77-based algorithm
    
- Fast, low-memory, optimized for short strings
    
- Typically 30‚Äì50% compression ratio
    

Since PostgreSQL 14+, you can also choose:

### ‚úî **LZ4**

- Much faster
    
- Lower CPU cost
    
- Slightly worse compression ratio than pglz
    

---

**What data types are compressed?**

Anything that is **varlena** (variable-length):

- `TEXT`
    
- `VARCHAR(n)` and `VARCHAR`
    
- `BYTEA`
    
- `JSONB`
    
- `XML`
    
- arrays
    
- composite types
    

NOT compressed:

- fixed-length types (INT, BIGINT)
    
- small varlenas (Postgres only compresses if value > ~1 KB)
    

---

**How compression actually works inside a tuple**

PostgreSQL stores variable-length values as **varlena objects**:

`+------------+--------------------+ | length hdr |  compressed data   | +------------+--------------------+`

When inserting:

- Step A ‚Äî Check size:

If the column value is > `toast_tuple_target` (typically 2 KB):

- Step B ‚Äî Try pglz or LZ4 compression:

It calls internal C functions:

`pglz_compress(val, compressed_val) lzcompress(val)`

Compression must satisfy:

- **Must reduce size** by at least **25%**
    
    - If not ‚Üí ‚Äúnot worth it‚Äù ‚Üí store uncompressed
        
    - Prevents wasting CPU on poorly compressible values
        

- Step C ‚Äî Store compressed varlena

If compression succeeds, Postgres marks the varlena as:

- 1-byte header flag: `VARATT_EXTERNAL_COMPRESSED`
    
- Stores compressed blob
    

Example (simplified):

Before:

`"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa" (40 bytes)`

After:

`pglz("aaaa...", 6 bytes)`

Resulting varlena stored inline is much smaller.

---

**If still too big ‚Üí go TOAST**

If the **tuple still does not fit into page**:

PostgreSQL moves the compressed varlena into a **TOAST table**, storing:

- **rowid**
    
- **chunk sequence**
    
- **compressed data**
    

The main page only stores:

`[TOAST pointer]`

The pointer is ~18 bytes.

---

**Example: 100 KB JSONB INSERT**

### Insert:

`INSERT INTO bigtable(data) VALUES (jsonb);`

### What happens:

1. Tuple is > 8 KB ‚Üí too big
    
2. PostgreSQL tries to pglz/LZ4 compress JSONB
    
3. Suppose compression shrinks it to 40 KB
    
4. Still too large ‚Üí move to TOAST
    
5. Store in ~20 chunks (2 KB each)
    
6. Heap row stores only the compressed TOAST pointer
    

---

**Why compress BEFORE TOASTing?**

Because:

‚úî Fewer TOAST chunks

(Chunk count ‚Üì = faster reads/writes)

‚úî Less disk I/O

(Compressed chunks are smaller)

‚úî Better cache usage

(smaller values = fewer pages loaded)

---

**Can you disable compression?**

Yes. Each column can define storage mode:

- `PLAIN` ‚Üí no compression, no TOAST
    
- `EXTENDED` ‚Üí compression + TOAST
    
- `EXTERNAL` ‚Üí no compression, but can move to TOAST
    
- `MAIN` ‚Üí attempt to keep inline, avoid TOAST
    

Example:

`ALTER TABLE t ALTER COLUMN c SET STORAGE PLAIN;`

### üü° PostgreSQL ‚Äì TOAST (The Oversized-Attribute Storage Technique)

Postgres _absolutely_ allows rows > 8 KB.  
It solves this with **TOAST tables**.

### How it works:

- Postgres stores only a **small pointer (toast header)** in the main heap page.
    
- Large values (TEXT, JSONB, BYTEA, arrays, etc.) are moved to:
    
    - **TOAST overflow pages**
        
    - Stored as chunks (2 KB each)
        
    - Stored inside a separate table: `pg_toast.<oid>`
        

### Compression order:

1. Try to **compress** column
    
2. If still too large ‚Üí move to TOAST
    
3. Store only a reference in the row
    

### Result:

‚úî PostgreSQL supports values up to **1 GB per field**  
‚úî Page size does NOT limit row size  
‚úî Only small metadata stays in main page

---

### üü° MySQL (InnoDB) ‚Äì Overflow Pages & Off-page Storage

InnoDB stores rows in **index-organized** B+Tree (clustered index).

Large variable-length columns (TEXT, BLOB, LONGTEXT, LONGBLOB) behave like:

### ‚öô How overflow works:

- InnoDB stores the first **768 bytes** of a long column **inline inside the page**
    
- The remaining part goes to:
    
    - **Overflow pages (off-page storage)**
        

The page then stores a **20-byte pointer** to the overflow chain.

### Example:

If you insert a 100 KB TEXT:

- First 768 bytes ‚Üí inline in the clustered index page
    
- Remaining 99 KB ‚Üí stored in overflow pages
    
- B+Tree leaf node points to them
    

### Notes:

- MySQL supports values up to **4 GB** per column
    
- Unlike Postgres, only TEXT/BLOB overflow; normal VARCHAR must fit the page

---

## 10. Fixed-length Attributes

Types like:

- `INT`
    
- `BIGINT`
    
- `FLOAT`
    
- `DATE`
    
- `BOOLEAN`
    
- small enums
    

These are stored after the variable-length area.

### Example

If your row is:

`id INT = 1 age INT = 25`

These go into the fixed-length zone:

`00 00 00 01   (id = 1) 00 00 00 19   (age = 25)`

Fixed-length fields always take the same number of bytes, no matter the value.

---

## 11. Padding / Alignment
![[Screenshot 2025-12-08 at 23.07.01.png]]
![[Screenshot 2025-12-08 at 23.07.27.png]]
![[Screenshot 2025-12-08 at 23.07.48.png]]


CPUs access memory faster when values are aligned to:

- 2 bytes
    
- 4 bytes
    
- 8 bytes boundaries
    

Databases add **padding** so fixed-length fields start aligned.

### Example

Suppose:

- varlen part is 5 bytes
    
- integer requires 4-byte alignment
    

DB adds **3 padding bytes** so the integer starts at an address divisible by 4.

`Varlen: 5 bytes Padding: 3 bytes Fixed: INT (4 bytes)`

This makes the tuple longer but improves CPU performance.
## 1. `SELECT pg_column_size(ROW(1,2,3));` ‚Üí 36 bytes

### Types inferred:

`1 ‚Üí int4 2 ‚Üí int4 3 ‚Üí int4`

### Per-field sizes:

- `int4` = **4 bytes**
    
- 3 fields ‚Üí 12 bytes of actual data
    

### Tuple metadata:

|Component|Size|
|---|---|
|Heap tuple header|23 bytes|
|Alignment padding|1 byte|
|Data|12 bytes|
|**Total**|**36 bytes**|

‚úî Why 1 byte of padding?  
PostgreSQL aligns 4-byte integers to 4-byte boundaries after the 23-byte tuple header.

So:

`23 (header) + 1 padding = 24 ‚Üí aligned to 4 bytes   24 + 12 (data) = 36`

---

## 2. `SELECT pg_column_size(ROW(1::int2,2::int2,3::int2));` ‚Üí 30 bytes

### Types:

All `int2` (smallint = 2 bytes)

### Data:

`2 + 2 + 2 = 6 bytes`

### Alignment:

Postgres aligns **int2 to 2-byte boundaries**.

Tuple header: 23 bytes  
23 is odd ‚Üí 1 byte padding to align to 2 bytes.

Total:

`23 + 1 + 6 = 30 bytes`

‚úî No alignment required between fields because `int2` only needs 2-byte alignment.

---

## 3. `SELECT pg_column_size(ROW('a'::char, 2::int2, 'b'::char, 4::int4, 'c'::char, 8::int8));` ‚Üí 48 bytes

Now rows have mixed alignment requirements:

|Field|Type|Size|Alignment|
|---|---|---|---|
|'a'|char(1)|1|1 byte|
|2|int2|2|2 bytes|
|'b'|char(1)|1|1 byte|
|4|int4|4|**4 bytes**|
|'c'|char(1)|1|1 byte|
|8|int8|8|**8 bytes** (biggest alignment)|

### Alignment rules:

The moment you hit `int4` and `int8`, PostgreSQL inserts padding to align the memory offset:

Breakdown:

|Description|Size|
|---|---|
|Tuple header|23|
|Align to 2 bytes (because next field int2)|+1|
|char(1)|+1|
|align to 2 bytes|+1|
|int2|+2|
|char(1)|+1|
|align to 4 bytes (for int4)|+1|
|int4|+4|
|char(1)|+1|
|align to 8 bytes (for int8)|+3|
|int8|+8|
|**Total**|**48 bytes**|

‚úî Most bloat comes from aligning before int4 and int8.

---

## 4. `SELECT pg_column_size(ROW(8::int8, 4::int4, 2::int2, 'a'::char, 'b'::char, 'c'::char));` ‚Üí 44 bytes

Same fields as example #3, but **different order**, therefore different padding.

This is the important lesson:

### **Changing field order changes total tuple size because alignment changes.**

Breakdown:

|Field|Type|Alignment|Notes|
|---|---|---|---|
|int8|8 bytes|aligned immediately after header||
|int4|4 bytes|naturally aligned after int8||
|int2|2 bytes|aligned||
|char|1 byte|no alignment needed||
|char|1 byte|no alignment needed||
|char|1 byte|no alignment needed||
### Final calculation

| Description                         | Size         |
| ----------------------------------- | ------------ |
| Tuple header                        | 23           |
| align to 8 bytes (first field int8) | +1           |
| int8                                | +8           |
| int4                                | +4           |
| int2                                | +2           |
| char(1)                             | +1           |
| char(1)                             | +1           |
| char(1)                             | +1           |
| **Total**                           | **44 bytes** |
![[Screenshot 2025-12-10 at 22.49.36.png]]
![[Screenshot 2025-12-10 at 22.51.02.png]]
![[Screenshot 2025-12-10 at 22.51.19.png]]

![[Screenshot 2025-12-08 at 23.11.28.png]]
![[Screenshot 2025-12-08 at 23.23.20.png]]
![[Screenshot 2025-12-08 at 23.23.55.png]]

# üü¶ 1. Two Big Categories of Numeric Types

## ‚úî **1. Fixed-precision (binary fixed-size)**

Examples:

- `INT`, `BIGINT`
    
- `FLOAT`, `DOUBLE` (not fixed decimal precision, but fixed binary size)
    
- Oracle `NUMBER(10)` _if fits in machine integer_ (special case)
    
- MySQL `INT`, `BIGINT`
    

Features:

- Stored in **fixed number of bytes**
    
- Arithmetic is done using **native CPU integer or float ALUs**, so extremely fast
    
- Limited precision (32/64 bits)
    

---

## ‚úî **2. Variable-precision (arbitrary precision / decimal)**

Examples:

- PostgreSQL `NUMERIC(p,s)`
    
- MySQL `DECIMAL(p,s)`
    
- Oracle `NUMBER(p,s)`
    
- Java BigDecimal (same concept)
    

Features:

- Stores **an arbitrary number of decimal digits**
    
- NOT handled by CPU ALU
    
- DB performs math using **software routines**
    
- Much slower (10x‚Äì200x slower than integer arithmetic)
    
- Used for money, finance, user-entered values, exact decimals
    

---

# üü¶ 2. Why CPU cannot handle DECIMAL natively

Because the CPU has only:

- 64-bit **integer ALU**
    
- 64-bit / 80-bit **floating-point FPU**
    

Neither supports exact decimal math.

So variable-precision numerics are implemented **in software**, using:

- Base-10 or base-100 representation
    
- Arrays of digits
    
- Manual multiplication/division routines
    
- Round/floor logic
    

üëâ That‚Äôs why DECIMAL/NUMERIC is slow.

---

# üü¶ 3. How databases store DECIMAL/NUMERIC internally

## ‚úî PostgreSQL `NUMERIC(p,s)` internal format

Postgres stores NUMERIC as:

`struct NumericVar {   int32 ndigits;     // number of base-10000 digits   int32 weight;      // position of decimal point   int32 sign;        // positive / negative   int16 *digits;     // array of base-10000 digits }`

Meaning:

- Each element in `digits[]` is **0‚Äì9999**
    
- 4 decimal digits per ‚Äúdigit‚Äù
    
- It grows or shrinks depending on the number‚Äôs precision ‚Üí **variable length**
    

### Example: Storing 123456.78

Split into base-10k:

`123456.78 = [12][3456] . [7800]`

Stored as:

`digits = {12, 3456, 7800} weight = 1   (first digit is 10^4)`

---

## ‚úî MySQL `DECIMAL(p,s)` internal storage

MySQL packs digits into:

- groups of 9 decimal digits
    
- using 4 bytes per group
    

Example: `DECIMAL(15,5)` ‚Üí max value = `999999999.99999`

Stored as:

`[000999999][99999_____]    4 bytes     3 bytes`

Using fixed-size groups ‚Äî so the total storage is:

`INT(9)*4 bytes + fractional groups`

---

## ‚úî Oracle `NUMBER(p,s)` internal storage

Oracle uses **base-100** (each byte = 2 decimal digits):

`0xC2 0x34 0x56 0x12`

is interpreted as:

- C2 ‚Üí sign + exponent
    
- 34 ‚Üí "34"
    
- 56 ‚Üí "56"
    
- 12 ‚Üí "12"
    

Variable number of bytes depending on value length.

---

# üü¶ 4. How CPU performs arithmetic on DECIMAL

Since CPU cannot do base-10 arithmetic natively:

### To add NUMERIC values:

1. DB aligns weights
    
2. Converts to same scale
    
3. Adds digits one by one (like manual math)
    
4. Normalizes carry
    
5. Applies rounding rules
    
6. Writes new digit array
    

### To multiply:

Nested loops:

`for i in digitsA:   for j in digitsB:     out[i+j] += digitsA[i] * digitsB[j]`

### CPU cost:

- `INT64` addition: **1 cycle**
    
- `NUMERIC` addition: **50‚Äì500 cycles**
    
- `NUMERIC` multiplication: **200‚Äì2000 cycles**
    

This is why DBs avoid NUMERIC in hot paths.

---

# üü¶ 5. Why DBs support both types

|Type|Speed|Precision|Use case|
|---|---|---|---|
|INT / BIGINT|‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê|Exact|IDs, counters|
|FLOAT / DOUBLE|‚≠ê‚≠ê‚≠ê‚≠ê|approx|scientific, analytics|
|DECIMAL / NUMERIC|‚≠ê|exact decimal|money, finance|

---

# üü¶ 6. Real DB Examples

## ‚úî Example in PostgreSQL

`CREATE TABLE payment(    amount NUMERIC(18,2) );`

Insert:

`INSERT INTO payment VALUES(1234567.89);`

Stored internally:

`digits = [123, 4567, 8900] weight = 1 scale  = 2`

Arithmetic:

`amount + 10.00 ‚Üí convert 10.00 to same scale ‚Üí align weight ‚Üí add digit arrays ‚Üí recalc carry`

---

## ‚úî Example in MySQL

`DECIMAL(10,4)` uses:

- integer part = 6 digits ‚Üí 4 bytes
    
- fractional part = 4 digits ‚Üí 4 bytes
    

Total: **8 bytes**, but not CPU-native.

---

# üü¶ 7. Why fixed-precision types are faster

Example: `BIGINT` addition:

`uint64_t x = a + b;`

- 1 CPU instruction
    
- fits in register
    
- no branching, no memory allocation
    

Example: `NUMERIC(18,2)` addition:

- Many CPU operations
    
- Requires malloc for digit arrays
    
- Requires normalization
    
- Requires decimal rounding
    
- Cannot use CPU ALU
    

This is why **financial systems use NUMERIC**, but **analytics systems avoid it**.
![[Screenshot 2025-12-10 at 22.20.36.png]]
![[Screenshot 2025-12-10 at 22.58.40.png]]![[Screenshot 2025-12-10 at 23.17.49.png]]![[Screenshot 2025-12-10 at 23.24.27.png]]![[Screenshot 2025-12-10 at 23.26.18.png]]

![[Screenshot 2025-12-10 at 23.28.23.png]]![[Screenshot 2025-12-10 at 23.34.16.png]]
## **Solution: Dictionary Compression**

Instead of storing long strings directly in the tuple, store a **small integer ID**.

### Step 1 ‚Äî Build a dictionary (unique values)

Example column:

`"USA" "Vietnam" "USA" "France" "Vietnam"`

Dictionary created:

|ID|Value|
|---|---|
|1|USA|
|2|Vietnam|
|3|France|

### Step 2 ‚Äî Replace column values with a **fixed 32-bit integer**

Column becomes:

`1 2 1 3 2`

Now every value is **just 4 bytes** (32-bit int), no matter how long the original string is.

![[Screenshot 2025-12-10 at 23.34.36.png]]
![[Screenshot 2025-12-10 at 23.35.55.png]]
![[Screenshot 2025-12-10 at 23.39.20.png]]
![[Screenshot 2025-12-10 at 23.41.28.png]]
https://www.youtube.com/watch?v=1j8SdS7s_NY&t=705s
### Parquet ‚Äî The #1 Data Lake Format

### **1Ô∏è‚É£ Columnar Storage**

- Stores data **by column**, not by row.
    
- Scans only needed columns ‚Üí **10√ó‚Äì100√ó less I/O**.
    

### **2Ô∏è‚É£ High Compression**

- Per-column compression: RLE, dictionary, delta, bit-packing, Snappy, ZSTD.
    
- Similar values ‚Üí great compression ‚Üí **CSV 1GB ‚Üí Parquet ~120MB**.
    

### **3Ô∏è‚É£ Predicate Pushdown**

- Stores min/max/null stats per column.
    
- Engines skip entire row groups ‚Üí **huge speedups**.
    

### **4Ô∏è‚É£ Self-Describing Schema**

- Schema, types, encoding, stats all inside the file.
    
- Supports schema evolution.
    

### **5Ô∏è‚É£ Vectorized Execution**

- Columnar blocks map to SIMD ‚Üí **CPU processes thousands of values at once**.
    

### **6Ô∏è‚É£ Row Groups & Pages**

- Organized for **random access**, skipping, parallel reads.
    

### **7Ô∏è‚É£ Splittable**

- Can be split at row group boundaries ‚Üí perfect for distributed systems.
    

### **8Ô∏è‚É£ Immutable-Friendly**

- Write-once, append-only ‚Üí ideal for S3/GCS/HDFS data lakes.
    

### **9Ô∏è‚É£ Widest Ecosystem Support**

- Works with Spark, Trino, BigQuery, DuckDB, Snowflake, Athena, etc.

![[Screenshot 2025-12-10 at 23.47.42.png]]![[Screenshot 2025-12-10 at 23.51.09.png]]![[Screenshot 2025-12-10 at 23.53.15.png]]
### üåü Summary Table

|Granularity|What is compressed|Systems|Pros|Cons|
|---|---|---|---|---|
|**Block-level**|Entire page|InnoDB, PG, SQLite|Great compression|Must decompress whole page|
|**Tuple-level**|Entire row|PostgreSQL, SQL Server|Good for OLTP|Can‚Äôt use cross-row redundancy|
|**Attribute-level**|Large individual values|PG TOAST, MySQL LOB|Only compress big fields|Overflow reads|
|**Column-level**|Whole column|Parquet, ORC, ClickHouse|Best compression & speed|Bad for OLTP|
![[Screenshot 2025-12-10 at 23.57.07.png]]![[Screenshot 2025-12-11 at 00.03.59.png]]
![[Screenshot 2025-12-11 at 00.05.00.png]]![[Screenshot 2025-12-11 at 00.07.42.png]]
![[Screenshot 2025-12-11 at 00.11.33.png]]
![[Screenshot 2025-12-11 at 00.12.37.png]]
![[Screenshot 2025-12-11 at 00.13.53.png]]
# Topic 1: Data Pipeline Orchestration with Apache Airflow
- **Question**: Walk through the structure of one of your complex Airflow DAGs. How did you incorporate PySpark tasks? 
	- **Expected Answer (Expanded)**: A complex DAG might start with a sensor task (e.g., ExternalTaskSensor or FileSensor) to wait for upstream data arrival, like ad log files in S3 or Kafka topics signaling completion. Then, a PythonOperator for lightweight preprocessing (e.g., validating file schemas), followed by a SparkSubmitOperator for heavy PySpark transformations—submitting a .py script with args like --master yarn --executor-memory 4g. Dependencies are chained with >> or BitShiftOperator for parallelism (e.g., multiple branches for different ad campaigns). Finally, a BashOperator or PythonOperator for loading to ClickHouse and notifying Metabase. In my Zalo Ads work, a DAG processed daily ad impressions: Ingestion >> PySpark aggregation (e.g., groupBy campaign_id, sum(clicks)) >> Load, with XCom pulls for dynamic params like date ranges. Idempotency via task_instance.xcom_pull to check prior runs, ensuring reruns don't duplicate data for DS model training.
- **Question**: How did you achieve 95%+ SLA compliance in your Airflow setups? What monitoring did you implement? 
	- **Expected Answer (Expanded)**: SLAs were enforced using Airflow's built-in SLA mechanism—defining sla=timedelta(hours=2) per task and sla_miss_callback to trigger alerts (e.g., Slack webhook with details like task_id, execution_date). Retries were key: default_retries=3 with exponential retry_delay (e.g., timedelta(minutes=5 * (2 ** retry_number))). For Ads pipelines, where delays could impact real-time bidding DS models, I added custom health checks via PythonOperator running SQL queries on ClickHouse to verify data freshness post-load. Monitoring: Airflow's Flower UI for task queues, plus StatsD exporter pushing metrics (e.g., task_duration, queue_length) to Prometheus/Grafana dashboards. Alerts on >90% CPU or task failures >5%. This combo kept SLAs at 95%+, reducing biz team complaints about stale Metabase reports from weekly to near-zero.
- **Question**: Explain how you handled dynamic DAGs or parameterized workflows in Airflow for varying data volumes. 
	- **Expected Answer (Expanded)**: For dynamic behavior, I used Jinja templating with macros like {{ ds }} for dates or {{ params.volume_threshold }} pulled from DAG config. BranchPythonOperator decided paths: e.g., if data volume >1TB (checked via PySpark count()), branch to high-resource Spark job; else, lightweight Python task. In Ads at Zalo, pipelines varied by campaign scale—parameterized via JSON configs in Variables or Connections, allowing DS teams to override via Airflow API triggers (e.g., airflow dags trigger --conf '{"campaign_id":123}'). For TMA test scripts, I used SubDAGOperator for reusable modules like data validation. Trade-off: Dynamics add flexibility but complexity; I mitigated with DAG linting in CI to catch template errors early.
- **Question**: What challenges arose with task dependencies in 50+ DAGs, and how did you optimize them? 
	- **Expected Answer (Expanded)**: With 50+ DAGs, cross-DAG dependencies caused bottlenecks—e.g., one DAG waiting on another's completion led to cascading delays in Ads reporting. Used ExternalTaskSensor with external_dag_id/task_id to sync, but optimized by grouping into meta-DAGs with TriggerDagRunOperator for orchestration. Fan-out/in: DummyOperator as join points. For optimization, pools limited concurrent Spark tasks (e.g., pool=spark_pool, slots=10) to avoid cluster overload. Priority_weight tuned for biz-critical paths (e.g., DA dashboards > experimental DS jobs). In practice, this cut average DAG runtime by 20% during peak ad traffic, preventing scheduler overload (monitored via scheduler_heartbeat_sec=5).
- **Question**: Describe integrating Airflow with Kafka for triggering streaming-related tasks.
	- **Expected Answer (Expanded)**: Integration via KafkaSensor (from airflow.providers.apache.kafka) polling a topic for messages (e.g., ad event completion signals). On poke success, it pushes XCom (e.g., message payload as params) to downstream tasks like SparkStreamingOperator. For at-least-once semantics, committed offsets manually in a callback. In Zalo Ads, this triggered near-real-time aggregations: Kafka topic 'ad_events' → Sensor → PySpark job aggregating clicks for DS serving. If no built-in sensor, custom PythonOperator with kafka-python client. Trade-off: Polling adds latency; for high-volume, used webhook from Kafka consumer to Airflow API (trigger_dag). Ensured no duplicate triggers via unique message IDs in XCom.
- **Question**: How did you manage secrets and configurations in your Airflow DAGs securely?
	- **Expected Answer (Expanded)**: Secrets via Airflow Connections (e.g., conn_id='clickhouse_prod' with encrypted password using Fernet key). In code: from airflow.hooks.base import BaseHook; hook = BaseHook.get_connection('clickhouse_prod'). Variables for non-secrets like API endpoints, stored in Metadata DB or AWS SSM backend for encryption. No hardcoding—always hook.get_extra() or Variable.get('key'). In Ads team, this secured DS access to sensitive bid data; audited via Airflow logs. For TMA projects, used environment variables injected via Kubernetes secrets. Best practice: Rotate Fernet keys periodically, and use RBAC to limit operator access.
- **Question**: Discuss scaling Airflow for high-throughput (e.g., for 2 TB/day integrations). What executor did you use?
	- **Expected Answer (Expanded)**: For scale, switched to CeleryExecutor with Redis backend (broker_url=redis://host:6379/0) for distributed task queuing, supporting 100+ workers. KubernetesExecutor for dynamic pods in cloud. Tuned scheduler (scheduler_heartbeat_sec=1, parallelism=128) and database (PostgreSQL with vacuuming). In Zalo's 2 TB/day logs, this handled 50+ DAGs by auto-scaling workers via K8s HPA on CPU>70%. Trade-off: Celery adds Redis dependency but enables horizontal scale; LocalExecutor suffices for small setups but bottlenecks at high throughput. Monitored with Prometheus for worker health, preventing OOM by resource limits.
- **Question**: How did you implement error notification and recovery in your DAGs? 
	- **Expected Answer (Expanded)**: on_failure_callback per task/DAG to call a function sending Slack/Email with context (e.g., task_instance, exception). Recovery: retry_exponential_backoff=True, with max_retries=5. For non-retriable, marked failed and triggered manual rerun via CLI (airflow tasks clear). In Ads, added custom rollback: e.g., PythonOperator to delete partial ClickHouse inserts on failure. Catchup=False for streaming DAGs to avoid backlog. Example: If PySpark OOM, callback logs spark.driver.log and alerts DS team. Reduced downtime for biz dashboards from hours to minutes.
- **Question**: What testing strategies did you use for Airflow DAGs before production?
	- **Expected Answer (Expanded)**: Unit tests with pytest-airflow: Mock tasks (e.g., patch 'airflow.operators.spark.SparkSubmitOperator') and assert dependencies. Integration: LocalExecutor in Docker compose, running 'airflow dags test my_dag' with sample data. For Ads pipelines, Great Expectations in a task for data validation. CI via GitLab: Stages for lint (airflow-dag-lint), unit, and smoke tests (backfill on historical dates). In TMA test script project, added end-to-end tests simulating failures. Coverage >80%; this caught issues like missing imports before deploy, ensuring 95% SLA.
- **Question**: How would you version-control and deploy updates to your 50+ DAGs without downtime? 
	- **Expected Answer (Expanded)**: Git for DAG files in a monorepo, with branches for features. Deploy: CI pipeline syncs to Airflow server via rsync or git pull in a webhook. For zero-downtime, new DAGs get versioned IDs (e.g., ads_pipeline_v2), run in parallel with v1 until validation (compare outputs via sensor). Pause/resume old DAG post-switch. In K8s, rolling updates with new image tags. In Zalo, this allowed hotfixes for DS pipelines without interrupting biz reports—e.g., add a task mid-DAG by redeploying and clearing downstream. Trade-off: Versioning bloats scheduler, so prune old DAGs via airflow dags delete.


# Topic 1: Data Pipeline Orchestration with Apache Airflow
- **Question**: Walk through the structure of one of your complex Airflow DAGs. How did you incorporate PySpark tasks? 
	- **Expected Answer (Expanded)**: A complex DAG might start with a sensor task (e.g., ExternalTaskSensor or FileSensor) to wait for upstream data arrival, like ad log files in S3 or Kafka topics signaling completion. Then, a PythonOperator for lightweight preprocessing (e.g., validating file schemas), followed by a SparkSubmitOperator for heavy PySpark transformations—submitting a .py script with args like --master yarn --executor-memory 4g. Dependencies are chained with >> or BitShiftOperator for parallelism (e.g., multiple branches for different ad campaigns). Finally, a BashOperator or PythonOperator for loading to ClickHouse and notifying Metabase. In my Zalo Ads work, a DAG processed daily ad impressions: Ingestion >> PySpark aggregation (e.g., groupBy campaign_id, sum(clicks)) >> Load, with XCom pulls for dynamic params like date ranges. Idempotency via task_instance.xcom_pull to check prior runs, ensuring reruns don't duplicate data for DS model training.
- **Question**: How did you achieve 95%+ SLA compliance in your Airflow setups? What monitoring did you implement? 
	- **Expected Answer (Expanded)**: SLAs were enforced using Airflow's built-in SLA mechanism—defining sla=timedelta(hours=2) per task and sla_miss_callback to trigger alerts (e.g., Slack webhook with details like task_id, execution_date). Retries were key: default_retries=3 with exponential retry_delay (e.g., timedelta(minutes=5 * (2 ** retry_number))). For Ads pipelines, where delays could impact real-time bidding DS models, I added custom health checks via PythonOperator running SQL queries on ClickHouse to verify data freshness post-load. Monitoring: Airflow's Flower UI for task queues, plus StatsD exporter pushing metrics (e.g., task_duration, queue_length) to Prometheus/Grafana dashboards. Alerts on >90% CPU or task failures >5%. This combo kept SLAs at 95%+, reducing biz team complaints about stale Metabase reports from weekly to near-zero.
- **Question**: Explain how you handled dynamic DAGs or parameterized workflows in Airflow for varying data volumes. 
	- **Expected Answer (Expanded)**: For dynamic behavior, I used Jinja templating with macros like {{ ds }} for dates or {{ params.volume_threshold }} pulled from DAG config. BranchPythonOperator decided paths: e.g., if data volume >1TB (checked via PySpark count()), branch to high-resource Spark job; else, lightweight Python task. In Ads at Zalo, pipelines varied by campaign scale—parameterized via JSON configs in Variables or Connections, allowing DS teams to override via Airflow API triggers (e.g., airflow dags trigger --conf '{"campaign_id":123}'). For TMA test scripts, I used SubDAGOperator for reusable modules like data validation. Trade-off: Dynamics add flexibility but complexity; I mitigated with DAG linting in CI to catch template errors early.
- **Question**: What challenges arose with task dependencies in 50+ DAGs, and how did you optimize them? 
	- **Expected Answer (Expanded)**: With 50+ DAGs, cross-DAG dependencies caused bottlenecks—e.g., one DAG waiting on another's completion led to cascading delays in Ads reporting. Used ExternalTaskSensor with external_dag_id/task_id to sync, but optimized by grouping into meta-DAGs with TriggerDagRunOperator for orchestration. Fan-out/in: DummyOperator as join points. For optimization, pools limited concurrent Spark tasks (e.g., pool=spark_pool, slots=10) to avoid cluster overload. Priority_weight tuned for biz-critical paths (e.g., DA dashboards > experimental DS jobs). In practice, this cut average DAG runtime by 20% during peak ad traffic, preventing scheduler overload (monitored via scheduler_heartbeat_sec=5).
- **Question**: Describe integrating Airflow with Kafka for triggering streaming-related tasks.
	- **Expected Answer (Expanded)**: Integration via KafkaSensor (from airflow.providers.apache.kafka) polling a topic for messages (e.g., ad event completion signals). On poke success, it pushes XCom (e.g., message payload as params) to downstream tasks like SparkStreamingOperator. For at-least-once semantics, committed offsets manually in a callback. In Zalo Ads, this triggered near-real-time aggregations: Kafka topic 'ad_events' → Sensor → PySpark job aggregating clicks for DS serving. If no built-in sensor, custom PythonOperator with kafka-python client. Trade-off: Polling adds latency; for high-volume, used webhook from Kafka consumer to Airflow API (trigger_dag). Ensured no duplicate triggers via unique message IDs in XCom.
- **Question**: How did you manage secrets and configurations in your Airflow DAGs securely?
	- **Expected Answer (Expanded)**: Secrets via Airflow Connections (e.g., conn_id='clickhouse_prod' with encrypted password using Fernet key). In code: from airflow.hooks.base import BaseHook; hook = BaseHook.get_connection('clickhouse_prod'). Variables for non-secrets like API endpoints, stored in Metadata DB or AWS SSM backend for encryption. No hardcoding—always hook.get_extra() or Variable.get('key'). In Ads team, this secured DS access to sensitive bid data; audited via Airflow logs. For TMA projects, used environment variables injected via Kubernetes secrets. Best practice: Rotate Fernet keys periodically, and use RBAC to limit operator access.
- **Question**: Discuss scaling Airflow for high-throughput (e.g., for 2 TB/day integrations). What executor did you use?
	- **Expected Answer (Expanded)**: For scale, switched to CeleryExecutor with Redis backend (broker_url=redis://host:6379/0) for distributed task queuing, supporting 100+ workers. KubernetesExecutor for dynamic pods in cloud. Tuned scheduler (scheduler_heartbeat_sec=1, parallelism=128) and database (PostgreSQL with vacuuming). In Zalo's 2 TB/day logs, this handled 50+ DAGs by auto-scaling workers via K8s HPA on CPU>70%. Trade-off: Celery adds Redis dependency but enables horizontal scale; LocalExecutor suffices for small setups but bottlenecks at high throughput. Monitored with Prometheus for worker health, preventing OOM by resource limits.
- **Question**: How did you implement error notification and recovery in your DAGs? 
	- **Expected Answer (Expanded)**: on_failure_callback per task/DAG to call a function sending Slack/Email with context (e.g., task_instance, exception). Recovery: retry_exponential_backoff=True, with max_retries=5. For non-retriable, marked failed and triggered manual rerun via CLI (airflow tasks clear). In Ads, added custom rollback: e.g., PythonOperator to delete partial ClickHouse inserts on failure. Catchup=False for streaming DAGs to avoid backlog. Example: If PySpark OOM, callback logs spark.driver.log and alerts DS team. Reduced downtime for biz dashboards from hours to minutes.
- **Question**: What testing strategies did you use for Airflow DAGs before production?
	- **Expected Answer (Expanded)**: Unit tests with pytest-airflow: Mock tasks (e.g., patch 'airflow.operators.spark.SparkSubmitOperator') and assert dependencies. Integration: LocalExecutor in Docker compose, running 'airflow dags test my_dag' with sample data. For Ads pipelines, Great Expectations in a task for data validation. CI via GitLab: Stages for lint (airflow-dag-lint), unit, and smoke tests (backfill on historical dates). In TMA test script project, added end-to-end tests simulating failures. Coverage >80%; this caught issues like missing imports before deploy, ensuring 95% SLA.
- **Question**: How would you version-control and deploy updates to your 50+ DAGs without downtime? 
	- **Expected Answer (Expanded)**: Git for DAG files in a monorepo, with branches for features. Deploy: CI pipeline syncs to Airflow server via rsync or git pull in a webhook. For zero-downtime, new DAGs get versioned IDs (e.g., ads_pipeline_v2), run in parallel with v1 until validation (compare outputs via sensor). Pause/resume old DAG post-switch. In K8s, rolling updates with new image tags. In Zalo, this allowed hotfixes for DS pipelines without interrupting biz reports—e.g., add a task mid-DAG by redeploying and clearing downstream. Trade-off: Versioning bloats scheduler, so prune old DAGs via airflow dags delete.


# Topic 2: Streaming and Near-Real-Time Data Processing (Kafka/Spark/Scala)
- **Question**: Describe the architecture of your 2 TB/day centralized logs pipeline in Scala. What role did Kafka play, and how did you integrate it with Spark Streaming? 
	- **Expected Answer**: Kafka as the ingestion layer for durable, partitioned message queuing (e.g., topics partitioned by log source or timestamp for parallelism). Spark Streaming (or Structured Streaming) for processing: Micro-batches or continuous mode for transformations like filtering, aggregating (e.g., windowed counts on log events). Integration via Kafka connector (e.g., spark-sql-kafka-0-10), with consumer groups for offset management. He should mention Scala traits/objects for modular code, and how parallelism reduced latency (e.g., executor cores matching partitions).
- **Question**: How did you achieve parallel transformations in your Scala pipeline? Provide a code snippet example and explain the performance impact. 
	- **Expected Answer**: Used Spark's RDD/DataFrame APIs in Scala for mapPartitions or foreachPartition on streams. Example: stream.mapPartitions { iter => iter.map(transformLog) } with Scala futures for intra-partition concurrency. Impact: Reduced latency by distributing compute across clusters (e.g., from 1h to 5-10 min via 10-20x parallelism), mentioning shuffle costs and how he tuned spark.default.parallelism.
- **Question**: What challenges did you face with data skew in Kafka topics, and how did you mitigate them in your pipeline? 
	- **Expected Answer**: Skew from uneven log volumes per source leading to hot partitions. Mitigation: Custom partitioner in Kafka producer (Scala class extending Partitioner, hashing on balanced keys like source+timestamp). In Spark, repartition() or coalesce() post-ingest. He should quantify: e.g., reduced max task time from 30 min to even distribution, preventing OOM.
- **Question**: Explain how you ensured exactly-once semantics in your Kafka-Spark streaming setup. What configurations were key? 
	- **Expected Answer**: Enabled idempotent producers in Kafka (enable.idempotence=true), combined with Spark's output commit protocol in Structured Streaming (e.g., foreachBatch with transactional sinks). Offsets committed atomically via Kafka's transactional API. Key configs: acks=all, retries high, spark.streaming.kafka.enableExactlyOnceSemantics. He should note trade-offs: slight throughput hit but prevented duplicates in aggregations.
- **Question**: In Scala, how did you handle schema evolution for streaming logs in your pipeline? Give an example of Avro or JSON handling. 
	- **Expected Answer**: Used Schema Registry with Kafka (Confluent or similar) for Avro-serialized messages. In Scala, case classes for schemas, with Spark's from_avro/to_avro functions. Example: val schema = SchemaRegistryClient.getLatest("logs-schema"); evolve by backward-compatible fields. Handled via try-catch for mismatches, logging drifts. Ensured no downtime during schema changes.
- **Question**: Describe fault tolerance mechanisms in your Kafka pipeline. How did you recover from broker failures? 
	- **Expected Answer**: Kafka replication factor (e.g., 3) for high availability, ISR for in-sync replicas. In Spark, checkpointing streams to HDFS/S3 (spark.streaming.checkpoint.dir). Recovery: Auto-offset reset to earliest/latest, with Spark's WAL for exactly-once. He should explain leader election process and how it minimized his pipeline's downtime (e.g., <1 min recovery).
- **Question**: How did you monitor and tune latency in your streaming pipeline? What metrics and tools did you use? 
	- **Expected Answer**: Monitored via Kafka JMX (e.g., under-replicated partitions, request latency) and Spark UI (processing time per batch). Tools: Prometheus/Grafana for custom Scala metrics (e.g., using Micrometer). Tuning: Increased partitions, tuned batch.interval.ms (e.g., from 1s to 500ms), added more consumers. Result: 5-10 min end-to-end, with alerts on >90th percentile latency.
- **Question**: What role did Scala play in your pipeline's efficiency compared to Python? Provide a specific example from your work. 
	- **Expected Answer**: Scala's type safety and performance (JVM optimizations) over Python's dynamism. Example: Immutable collections (Seq/Vector) for thread-safe transformations vs. Python lists; used Akka actors for async processing if applicable. Efficiency: Faster compile-time checks reduced bugs, and JVM garbage collection tuning cut GC pauses in long-running streams.
- **Question**: How would you extend your pipeline to handle backpressure in high-volume scenarios? Explain with Kafka/Spark configs. 
	- **Expected Answer**: Kafka: max.in.flight.requests.per.connection=1 for order preservation. Spark: Backpressure enabled (spark.streaming.backpressure.enabled=true), dynamic allocation for executors. Rate limiting via spark.streaming.kafka.maxRatePerPartition. He should describe a scenario: e.g., during log spikes, throttled to prevent OOM, maintaining 5-10 min latency.
- **Question**: Discuss error handling in your streaming transformations. How did you manage retries and dead-letter queues? 
	- **Expected Answer**: In Scala, Try/Success/Failure pattern for transformations, with retries via exponential backoff (e.g., using Scala's Future.retry). Errors routed to DLQ Kafka topic (separate producer). Spark's foreachPartition for batch commits. Ensured idempotency to avoid duplicates on retries, reducing failure rates as in his HPC migration (from 10% to 5%).
*  **Question**: Describe your ClickHouse data model for ML feature serving and BI. Why did you choose specific engines? 
	* **Expected Answer**: 
		* For ML feature serving in Zalo Ads, I used the standard MergeTree engine on a single instance: e.g., CREATE TABLE ad_features (user_id UInt64, feature_vector Array(Float32), timestamp DateTime) ENGINE = MergeTree() PARTITION BY toYYYYMM(timestamp) ORDER BY (user_id, timestamp) SETTINGS index_granularity = 8192. This enabled efficient merging of inserts and fast primary key lookups for DS teams querying user features in real-time bidding models (e.g., SELECT feature_vector FROM ad_features WHERE user_id = 12345 LIMIT 1). 
		* For BI aggregations (e.g., campaign performance), I used AggregatingMergeTree: ENGINE = AggregatingMergeTree() with columns like sumState(clicks) for incremental rollups. Choice: MergeTree for its columnar storage, compression, and background merging on a single node—ideal for our 2 TB/day log volume without needing distribution, as the instance had sufficient resources (e.g., 128GB RAM, NVMe SSDs) to handle queries in-memory. Avoided ReplicatedMergeTree since it requires ZooKeeper and multiple nodes, adding unnecessary complexity/cost for our setup where HA wasn't mission-critical (we relied on daily backups and Airflow retries for resilience). 
		* Trade-off: Single instance means potential single-point failure, but it simplified ops and achieved 2–10× query speeds for DA dashboards; if scale grew, I'd migrate to Cluster setup. Alternatives like Memory engine were too volatile for persistent data.
- **Question**: How did you optimize storage in ClickHouse (30% reduction)? Give compression and partitioning examples. 
	- **Expected Answer (Expanded)**: Optimization started with column-specific codecs: e.g., ALTER TABLE ad_logs MODIFY COLUMN impressions UInt32 CODEC(Delta, LZ4) for delta-compressed counters, and Array(Float32) CODEC(T64, ZSTD) for feature vectors—ZSTD for high-ratio on repetitive data. Partitioning: PARTITION BY toYYYYMMDD(timestamp) for daily ad events, enabling easy drop/attach for maintenance without full rescans. TTL: SETTINGS ttl_only_replicated=1, TTL timestamp + INTERVAL 90 DAY DELETE for expiring old ad logs. In Ads at Zalo, this reduced storage from ~1PB projected to 700TB actual (30% savings) by compressing repetitive user_id strings with Gorilla codec and partitioning to minimize I/O for DS queries like SELECT * WHERE timestamp > '2024-01-01'. Trade-off: Fine-grained partitioning increases metadata overhead, so I balanced with granularity=8192; monitored via system.parts table to avoid too many parts (>10k active).
- **Question**: Explain materialized views in your ClickHouse setup. How did they speed up queries? **Expected Answer (Expanded)**: Materialized views pre-compute and store results incrementally: e.g., CREATE MATERIALIZED VIEW ad_daily_agg ENGINE = AggregatingMergeTree() POPULATE AS SELECT toDate(timestamp) as date, campaign_id, sumState(clicks) as clicks FROM ad_logs GROUP BY date, campaign_id. This view updates on inserts to the base table, using sumMerge for aggregations. In Zalo BI, it sped up DA queries like SELECT sumMerge(clicks) FROM ad_daily_agg WHERE date = '2024-09-01' from minutes to seconds (2–10× faster), as it avoided scanning raw logs (billions of rows). For ML, a view for feature stats (avg, stddev over windows). Trade-off: Views consume extra storage (10–20% overhead) and insert latency (+5–10ms), but I mitigated with asynchronous population and monitored via system.materialized_views. Best for frequent, expensive queries in Ads dashboards.
- **Question**: What indexing strategies did you use in PostgreSQL for 3x query speedup?
	- **Expected Answer (Expanded)**: In the Bronze/Silver/Gold architecture at TMA, I used composite B-tree indexes: e.g., CREATE INDEX idx_ad_events ON bronze_ad_events (timestamp, user_id) WHERE status = 'active'; GIN for JSONB features (CREATE INDEX idx_features_gin ON silver_features USING GIN (feature_json)); BRIN for time-series on large tables (CREATE INDEX idx_timestamp_brin ON gold_aggregates USING BRIN (timestamp) WITH (pages_per_range=128)). Denormalization: In Gold, flattened nested data to avoid joins, e.g., embedding campaign details. This achieved 3x speedup for queries like SELECT * FROM gold_aggregates WHERE timestamp BETWEEN '2023-05-01' AND '2023-08-01' GROUP BY campaign_id, reducing from 30s to 10s. Trade-off: Indexes slow inserts (by 20%), so I rebuilt them during low-traffic via REINDEX CONCURRENTLY; analyzed with pg_stat_user_indexes for unused ones. Tied to ELT pipelines for DA/BIZ reporting.
- **Question**: How did you handle high-write ingestion (thousands/sec) in ClickHouse without downtime? 
	- **Expected Answer (Expanded)**: Used buffered inserts: Client-side batching (e.g., via Python clickhouse-driver, insert_block_size=100000) and server buffers (SETTINGS insert_quorum=2 for consistency). Asynchronous mode: INSERT INTO ad_logs FORMAT JSONEachRow ASYNC to queue writes. For Ads logs from Kafka, integrated via Kafka engine table (ENGINE = Kafka() SETTINGS kafka_broker_list='brokers', kafka_topic_list='ad_events'). This sustained 1k–5k writes/sec with <1s latency. Downtime avoidance: Replicated setup with ALTER TABLE DETACH/ATTACH PARTITION during schema changes. Monitored insert rates via system.metrics (Query 'Insert'), alerting on queue buildup. Trade-off: Buffering risks data loss on crash, mitigated with WAL (write-ahead log) enabled. In Zalo, this ensured continuous DS feature serving without interruptions.
- **Question**: Discuss trade-offs between ClickHouse and PostgreSQL in your architectures.
	- **Expected Answer (Expanded)**: ClickHouse excels in OLAP: Columnar storage for fast scans/aggregations on large Ads datasets (e.g., 2 TB/day logs), but lacks full ACID (only atomic inserts). PostgreSQL for OLTP: ACID transactions for metadata (e.g., campaign configs in Bronze layer), with row-level locking. In Zalo, used ClickHouse for BI/ML queries (2–10× faster, 30% cheaper storage) but PostgreSQL for transactional Silver/Gold layers needing updates (e.g., UPDATE ad_status). Trade-offs: ClickHouse scales horizontally easier (add shards) but queries are approximate (no full joins); Postgres better for complex joins but slower at scale without extensions like TimescaleDB. Hybrid: Airflow DAGs ingested to Postgres first for validation, then to ClickHouse. Chose based on workload—OLAP-heavy for DS training pipelines.
- **Question**: How did you implement projections in ClickHouse, and what was the impact?
	- **Expected Answer (Expanded)**: Projections store alternate sort orders/materializations: e.g., ALTER TABLE ad_logs ADD PROJECTION proj_campaign (SELECT * ORDER BY campaign_id, timestamp); This pre-sorts data blocks for queries filtering on campaign_id without rescanning. Populated via OPTIMIZE TABLE ad_logs FINAL or incrementally. In Ads, it sped up DA queries like SELECT sum(clicks) GROUP BY campaign_id from hours to minutes (5–10×), as it skipped irrelevant blocks. Impact: Reduced CPU/I/O by 40%, but added 10–15% storage. Trade-off: Projections rebuild on schema changes, so I limited to 2–3 per table; monitored with system.projections. Tied to BI optimizations, ensuring Metabase dashboards loaded instantly for biz teams.
- **Question**: Describe vacuuming and maintenance in your PostgreSQL Gold layer. **Expected
	- Answer (Expanded)**: AUTOVACUUM tuned aggressively: ALTER TABLE gold_aggregates SET (autovacuum_vacuum_scale_factor = 0.05, autovacuum_analyze_scale_factor = 0.02) to trigger on 5% changes. Manual VACUUM ANALYZE during off-peak via cron/Airflow. For partitioned tables (e.g., by month: CREATE TABLE gold_aggregates PARTITION BY RANGE (timestamp)), detached old partitions for archiving. In TMA's unified Data Lake, this prevented bloat from frequent ELT updates, maintaining 3x query speeds. Impact: Reduced table size by 20% post-vacuum, fixed query plans with fresh stats. Trade-off: Vacuum can lock during full mode, so used VACUUM FREEZE for long-running; monitored pg_stat_all_tables for last_vacuum. Ensured no downtime for DA/BIZ access.
- **Question**: How did you query optimize for BI in ClickHouse (e.g., skip indexes)? **Expected Answer (Expanded)**: Skip indexes avoid reading unnecessary data blocks: e.g., ALTER TABLE ad_logs ADD INDEX idx_user_skip (user_id) TYPE minmax GRANULARITY 3; For blooms: TYPE bloom_filter(0.01) on strings. This skipped blocks where min/max or bloom didn't match filters, e.g., SELECT * WHERE user_id = 12345 scanned only relevant granules. In Zalo BI, optimized for Metabase queries on ad performance, reducing I/O by 50–70% and achieving 2–10× speedups. Trade-off: Indexes add insert overhead (+10ms) and storage (5%); I chose granularity=3 for balance (block size ~24k rows). Monitored effectiveness with EXPLAIN or system.query_log. For DS, combined with materialized views for feature queries.
- **Question**: What sharding/replication did you set up in ClickHouse for scalability? **Expected Answer (Expanded)**: Sharding via Distributed engine: CREATE TABLE ad_logs_dist ENGINE = Distributed('cluster_name', 'default', 'ad_logs', rand()); With 3 shards, data hashed by sharding_key (e.g., cityHash64(user_id)). Replication: ReplicatedMergeTree with ZooKeeper quorum (quorum=2). In Zalo's high-volume Ads (2 TB/day), this scaled reads/writes linearly—e.g., queries distributed across nodes, sustaining thousands/sec inserts. Failover: Automatic via replicas. Trade-off: Sharding complicates joins (use local tables + FINAL), and ZooKeeper is a SPOF, so used 3-node ZK ensemble. Monitored with clickhouse-keeper logs and system.clusters. For ML serving, ensured low-latency by colocating shards with compute nodes, reducing costs by 30% through efficient resource use.
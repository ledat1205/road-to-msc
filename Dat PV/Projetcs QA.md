# Topic 1: Data Pipeline Orchestration with Apache Airflow
- **Question**: Walk through the structure of one of your complex Airflow DAGs. How did you incorporate PySpark tasks? 
	- **Expected Answer (Expanded)**: A complex DAG might start with a sensor task (e.g., ExternalTaskSensor or FileSensor) to wait for upstream data arrival, like ad log files in S3 or Kafka topics signaling completion. Then, a PythonOperator for lightweight preprocessing (e.g., validating file schemas), followed by a SparkSubmitOperator for heavy PySpark transformations—submitting a .py script with args like --master yarn --executor-memory 4g. Dependencies are chained with >> or BitShiftOperator for parallelism (e.g., multiple branches for different ad campaigns). Finally, a BashOperator or PythonOperator for loading to ClickHouse and notifying Metabase. In my Zalo Ads work, a DAG processed daily ad impressions: Ingestion >> PySpark aggregation (e.g., groupBy campaign_id, sum(clicks)) >> Load, with XCom pulls for dynamic params like date ranges. Idempotency via task_instance.xcom_pull to check prior runs, ensuring reruns don't duplicate data for DS model training.
- **Question**: How did you achieve 95%+ SLA compliance in your Airflow setups? What monitoring did you implement? 
	- **Expected Answer (Expanded)**: SLAs were enforced using Airflow's built-in SLA mechanism—defining sla=timedelta(hours=2) per task and sla_miss_callback to trigger alerts (e.g., Slack webhook with details like task_id, execution_date). Retries were key: default_retries=3 with exponential retry_delay (e.g., timedelta(minutes=5 * (2 ** retry_number))). For Ads pipelines, where delays could impact real-time bidding DS models, I added custom health checks via PythonOperator running SQL queries on ClickHouse to verify data freshness post-load. Monitoring: Airflow's Flower UI for task queues, plus StatsD exporter pushing metrics (e.g., task_duration, queue_length) to Prometheus/Grafana dashboards. Alerts on >90% CPU or task failures >5%. This combo kept SLAs at 95%+, reducing biz team complaints about stale Metabase reports from weekly to near-zero.
- **Question**: Explain how you handled dynamic DAGs or parameterized workflows in Airflow for varying data volumes. 
	- **Expected Answer (Expanded)**: For dynamic behavior, I used Jinja templating with macros like {{ ds }} for dates or {{ params.volume_threshold }} pulled from DAG config. BranchPythonOperator decided paths: e.g., if data volume >1TB (checked via PySpark count()), branch to high-resource Spark job; else, lightweight Python task. In Ads at Zalo, pipelines varied by campaign scale—parameterized via JSON configs in Variables or Connections, allowing DS teams to override via Airflow API triggers (e.g., airflow dags trigger --conf '{"campaign_id":123}'). For TMA test scripts, I used SubDAGOperator for reusable modules like data validation. Trade-off: Dynamics add flexibility but complexity; I mitigated with DAG linting in CI to catch template errors early.
- **Question**: What challenges arose with task dependencies in 50+ DAGs, and how did you optimize them? 
	- **Expected Answer (Expanded)**: With 50+ DAGs, cross-DAG dependencies caused bottlenecks—e.g., one DAG waiting on another's completion led to cascading delays in Ads reporting. Used ExternalTaskSensor with external_dag_id/task_id to sync, but optimized by grouping into meta-DAGs with TriggerDagRunOperator for orchestration. Fan-out/in: DummyOperator as join points. For optimization, pools limited concurrent Spark tasks (e.g., pool=spark_pool, slots=10) to avoid cluster overload. Priority_weight tuned for biz-critical paths (e.g., DA dashboards > experimental DS jobs). In practice, this cut average DAG runtime by 20% during peak ad traffic, preventing scheduler overload (monitored via scheduler_heartbeat_sec=5).
- **Question**: Describe integrating Airflow with Kafka for triggering streaming-related tasks.
	- **Expected Answer (Expanded)**: Integration via KafkaSensor (from airflow.providers.apache.kafka) polling a topic for messages (e.g., ad event completion signals). On poke success, it pushes XCom (e.g., message payload as params) to downstream tasks like SparkStreamingOperator. For at-least-once semantics, committed offsets manually in a callback. In Zalo Ads, this triggered near-real-time aggregations: Kafka topic 'ad_events' → Sensor → PySpark job aggregating clicks for DS serving. If no built-in sensor, custom PythonOperator with kafka-python client. Trade-off: Polling adds latency; for high-volume, used webhook from Kafka consumer to Airflow API (trigger_dag). Ensured no duplicate triggers via unique message IDs in XCom.
- **Question**: How did you manage secrets and configurations in your Airflow DAGs securely?
	- **Expected Answer (Expanded)**: Secrets via Airflow Connections (e.g., conn_id='clickhouse_prod' with encrypted password using Fernet key). In code: from airflow.hooks.base import BaseHook; hook = BaseHook.get_connection('clickhouse_prod'). Variables for non-secrets like API endpoints, stored in Metadata DB or AWS SSM backend for encryption. No hardcoding—always hook.get_extra() or Variable.get('key'). In Ads team, this secured DS access to sensitive bid data; audited via Airflow logs. For TMA projects, used environment variables injected via Kubernetes secrets. Best practice: Rotate Fernet keys periodically, and use RBAC to limit operator access.
- **Question**: Discuss scaling Airflow for high-throughput (e.g., for 2 TB/day integrations). What executor did you use?
	- **Expected Answer (Expanded)**: For scale, switched to CeleryExecutor with Redis backend (broker_url=redis://host:6379/0) for distributed task queuing, supporting 100+ workers. KubernetesExecutor for dynamic pods in cloud. Tuned scheduler (scheduler_heartbeat_sec=1, parallelism=128) and database (PostgreSQL with vacuuming). In Zalo's 2 TB/day logs, this handled 50+ DAGs by auto-scaling workers via K8s HPA on CPU>70%. Trade-off: Celery adds Redis dependency but enables horizontal scale; LocalExecutor suffices for small setups but bottlenecks at high throughput. Monitored with Prometheus for worker health, preventing OOM by resource limits.
- **Question**: How did you implement error notification and recovery in your DAGs? 
	- **Expected Answer (Expanded)**: on_failure_callback per task/DAG to call a function sending Slack/Email with context (e.g., task_instance, exception). Recovery: retry_exponential_backoff=True, with max_retries=5. For non-retriable, marked failed and triggered manual rerun via CLI (airflow tasks clear). In Ads, added custom rollback: e.g., PythonOperator to delete partial ClickHouse inserts on failure. Catchup=False for streaming DAGs to avoid backlog. Example: If PySpark OOM, callback logs spark.driver.log and alerts DS team. Reduced downtime for biz dashboards from hours to minutes.
- **Question**: What testing strategies did you use for Airflow DAGs before production?
	- **Expected Answer (Expanded)**: Unit tests with pytest-airflow: Mock tasks (e.g., patch 'airflow.operators.spark.SparkSubmitOperator') and assert dependencies. Integration: LocalExecutor in Docker compose, running 'airflow dags test my_dag' with sample data. For Ads pipelines, Great Expectations in a task for data validation. CI via GitLab: Stages for lint (airflow-dag-lint), unit, and smoke tests (backfill on historical dates). In TMA test script project, added end-to-end tests simulating failures. Coverage >80%; this caught issues like missing imports before deploy, ensuring 95% SLA.
- **Question**: How would you version-control and deploy updates to your 50+ DAGs without downtime? 
	- **Expected Answer (Expanded)**: Git for DAG files in a monorepo, with branches for features. Deploy: CI pipeline syncs to Airflow server via rsync or git pull in a webhook. For zero-downtime, new DAGs get versioned IDs (e.g., ads_pipeline_v2), run in parallel with v1 until validation (compare outputs via sensor). Pause/resume old DAG post-switch. In K8s, rolling updates with new image tags. In Zalo, this allowed hotfixes for DS pipelines without interrupting biz reports—e.g., add a task mid-DAG by redeploying and clearing downstream. Trade-off: Versioning bloats scheduler, so prune old DAGs via airflow dags delete.


# Topic 2: Streaming and Near-Real-Time Data Processing (Kafka/Spark/Scala)
- **Question**: Describe the architecture of your 2 TB/day centralized logs pipeline in Scala. What role did Kafka play, and how did you integrate it with Spark Streaming? 
	- **Expected Answer**: Kafka as the ingestion layer for durable, partitioned message queuing (e.g., topics partitioned by log source or timestamp for parallelism). Spark Streaming (or Structured Streaming) for processing: Micro-batches or continuous mode for transformations like filtering, aggregating (e.g., windowed counts on log events). Integration via Kafka connector (e.g., spark-sql-kafka-0-10), with consumer groups for offset management. He should mention Scala traits/objects for modular code, and how parallelism reduced latency (e.g., executor cores matching partitions).
- **Question**: How did you achieve parallel transformations in your Scala pipeline? Provide a code snippet example and explain the performance impact. 
	- **Expected Answer**: Used Spark's RDD/DataFrame APIs in Scala for mapPartitions or foreachPartition on streams. Example: stream.mapPartitions { iter => iter.map(transformLog) } with Scala futures for intra-partition concurrency. Impact: Reduced latency by distributing compute across clusters (e.g., from 1h to 5-10 min via 10-20x parallelism), mentioning shuffle costs and how he tuned spark.default.parallelism.
- **Question**: What challenges did you face with data skew in Kafka topics, and how did you mitigate them in your pipeline? 
	- **Expected Answer**: Skew from uneven log volumes per source leading to hot partitions. Mitigation: Custom partitioner in Kafka producer (Scala class extending Partitioner, hashing on balanced keys like source+timestamp). In Spark, repartition() or coalesce() post-ingest. He should quantify: e.g., reduced max task time from 30 min to even distribution, preventing OOM.
- **Question**: Explain how you ensured exactly-once semantics in your Kafka-Spark streaming setup. What configurations were key? 
	- **Expected Answer**: Enabled idempotent producers in Kafka (enable.idempotence=true), combined with Spark's output commit protocol in Structured Streaming (e.g., foreachBatch with transactional sinks). Offsets committed atomically via Kafka's transactional API. Key configs: acks=all, retries high, spark.streaming.kafka.enableExactlyOnceSemantics. He should note trade-offs: slight throughput hit but prevented duplicates in aggregations.
- **Question**: In Scala, how did you handle schema evolution for streaming logs in your pipeline? Give an example of Avro or JSON handling. 
	- **Expected Answer**: Used Schema Registry with Kafka (Confluent or similar) for Avro-serialized messages. In Scala, case classes for schemas, with Spark's from_avro/to_avro functions. Example: val schema = SchemaRegistryClient.getLatest("logs-schema"); evolve by backward-compatible fields. Handled via try-catch for mismatches, logging drifts. Ensured no downtime during schema changes.
- **Question**: Describe fault tolerance mechanisms in your Kafka pipeline. How did you recover from broker failures? 
	- **Expected Answer**: Kafka replication factor (e.g., 3) for high availability, ISR for in-sync replicas. In Spark, checkpointing streams to HDFS/S3 (spark.streaming.checkpoint.dir). Recovery: Auto-offset reset to earliest/latest, with Spark's WAL for exactly-once. He should explain leader election process and how it minimized his pipeline's downtime (e.g., <1 min recovery).
- **Question**: How did you monitor and tune latency in your streaming pipeline? What metrics and tools did you use? 
	- **Expected Answer**: Monitored via Kafka JMX (e.g., under-replicated partitions, request latency) and Spark UI (processing time per batch). Tools: Prometheus/Grafana for custom Scala metrics (e.g., using Micrometer). Tuning: Increased partitions, tuned batch.interval.ms (e.g., from 1s to 500ms), added more consumers. Result: 5-10 min end-to-end, with alerts on >90th percentile latency.
- **Question**: What role did Scala play in your pipeline's efficiency compared to Python? Provide a specific example from your work. 
	- **Expected Answer**: Scala's type safety and performance (JVM optimizations) over Python's dynamism. Example: Immutable collections (Seq/Vector) for thread-safe transformations vs. Python lists; used Akka actors for async processing if applicable. Efficiency: Faster compile-time checks reduced bugs, and JVM garbage collection tuning cut GC pauses in long-running streams.
- **Question**: How would you extend your pipeline to handle backpressure in high-volume scenarios? Explain with Kafka/Spark configs. 
	- **Expected Answer**: Kafka: max.in.flight.requests.per.connection=1 for order preservation. Spark: Backpressure enabled (spark.streaming.backpressure.enabled=true), dynamic allocation for executors. Rate limiting via spark.streaming.kafka.maxRatePerPartition. He should describe a scenario: e.g., during log spikes, throttled to prevent OOM, maintaining 5-10 min latency.
- **Question**: Discuss error handling in your streaming transformations. How did you manage retries and dead-letter queues? 
	- **Expected Answer**: In Scala, Try/Success/Failure pattern for transformations, with retries via exponential backoff (e.g., using Scala's Future.retry). Errors routed to DLQ Kafka topic (separate producer). Spark's foreachPartition for batch commits. Ensured idempotency to avoid duplicates on retries, reducing failure rates as in his HPC migration (from 10% to 5%).
*  **Question**: Describe your ClickHouse data model for ML feature serving and BI. Why did you choose specific engines? 
	* **Expected Answer**: 
		* For ML feature serving in Zalo Ads, I used the standard MergeTree engine on a single instance: e.g., CREATE TABLE ad_features (user_id UInt64, feature_vector Array(Float32), timestamp DateTime) ENGINE = MergeTree() PARTITION BY toYYYYMM(timestamp) ORDER BY (user_id, timestamp) SETTINGS index_granularity = 8192. This enabled efficient merging of inserts and fast primary key lookups for DS teams querying user features in real-time bidding models (e.g., SELECT feature_vector FROM ad_features WHERE user_id = 12345 LIMIT 1). 
		* For BI aggregations (e.g., campaign performance), I used AggregatingMergeTree: ENGINE = AggregatingMergeTree() with columns like sumState(clicks) for incremental rollups. Choice: MergeTree for its columnar storage, compression, and background merging on a single node—ideal for our 2 TB/day log volume without needing distribution, as the instance had sufficient resources (e.g., 128GB RAM, NVMe SSDs) to handle queries in-memory. Avoided ReplicatedMergeTree since it requires ZooKeeper and multiple nodes, adding unnecessary complexity/cost for our setup where HA wasn't mission-critical (we relied on daily backups and Airflow retries for resilience). 
		* Trade-off: Single instance means potential single-point failure, but it simplified ops and achieved 2–10× query speeds for DA dashboards; if scale grew, I'd migrate to Cluster setup. Alternatives like Memory engine were too volatile for persistent data.
- **Question**: How did you optimize storage in ClickHouse (30% reduction)? Give compression and partitioning examples. 
	- **Expected Answer (Expanded)**: Optimization started with column-specific codecs: e.g., ALTER TABLE ad_logs MODIFY COLUMN impressions UInt32 CODEC(Delta, LZ4) for delta-compressed counters, and Array(Float32) CODEC(T64, ZSTD) for feature vectors—ZSTD for high-ratio on repetitive data. Partitioning: PARTITION BY toYYYYMMDD(timestamp) for daily ad events, enabling easy drop/attach for maintenance without full rescans. TTL: SETTINGS ttl_only_replicated=1, TTL timestamp + INTERVAL 90 DAY DELETE for expiring old ad logs. In Ads at Zalo, this reduced storage from ~1PB projected to 700TB actual (30% savings) by compressing repetitive user_id strings with Gorilla codec and partitioning to minimize I/O for DS queries like SELECT * WHERE timestamp > '2024-01-01'. Trade-off: Fine-grained partitioning increases metadata overhead, so I balanced with granularity=8192; monitored via system.parts table to avoid too many parts (>10k active).
- **Question**: Explain materialized views in your ClickHouse setup. How did they speed up queries? **Expected Answer (Expanded)**: Materialized views pre-compute and store results incrementally: e.g., CREATE MATERIALIZED VIEW ad_daily_agg ENGINE = AggregatingMergeTree() POPULATE AS SELECT toDate(timestamp) as date, campaign_id, sumState(clicks) as clicks FROM ad_logs GROUP BY date, campaign_id. This view updates on inserts to the base table, using sumMerge for aggregations. In Zalo BI, it sped up DA queries like SELECT sumMerge(clicks) FROM ad_daily_agg WHERE date = '2024-09-01' from minutes to seconds (2–10× faster), as it avoided scanning raw logs (billions of rows). For ML, a view for feature stats (avg, stddev over windows). Trade-off: Views consume extra storage (10–20% overhead) and insert latency (+5–10ms), but I mitigated with asynchronous population and monitored via system.materialized_views. Best for frequent, expensive queries in Ads dashboards.
- **Question**: What indexing strategies did you use in PostgreSQL for 3x query speedup?
	- **Expected Answer (Expanded)**: In the Bronze/Silver/Gold architecture at TMA, I used composite B-tree indexes: e.g., CREATE INDEX idx_ad_events ON bronze_ad_events (timestamp, user_id) WHERE status = 'active'; GIN for JSONB features (CREATE INDEX idx_features_gin ON silver_features USING GIN (feature_json)); BRIN for time-series on large tables (CREATE INDEX idx_timestamp_brin ON gold_aggregates USING BRIN (timestamp) WITH (pages_per_range=128)). Denormalization: In Gold, flattened nested data to avoid joins, e.g., embedding campaign details. This achieved 3x speedup for queries like SELECT * FROM gold_aggregates WHERE timestamp BETWEEN '2023-05-01' AND '2023-08-01' GROUP BY campaign_id, reducing from 30s to 10s. Trade-off: Indexes slow inserts (by 20%), so I rebuilt them during low-traffic via REINDEX CONCURRENTLY; analyzed with pg_stat_user_indexes for unused ones. Tied to ELT pipelines for DA/BIZ reporting.
- **Question**: How did you handle high-write ingestion (thousands/sec) in ClickHouse without downtime? 
	- **Expected Answer (Expanded)**: Used buffered inserts: Client-side batching (e.g., via Python clickhouse-driver, insert_block_size=100000) and server buffers (SETTINGS insert_quorum=2 for consistency). Asynchronous mode: INSERT INTO ad_logs FORMAT JSONEachRow ASYNC to queue writes. For Ads logs from Kafka, integrated via Kafka engine table (ENGINE = Kafka() SETTINGS kafka_broker_list='brokers', kafka_topic_list='ad_events'). This sustained 1k–5k writes/sec with <1s latency. Downtime avoidance: Replicated setup with ALTER TABLE DETACH/ATTACH PARTITION during schema changes. Monitored insert rates via system.metrics (Query 'Insert'), alerting on queue buildup. Trade-off: Buffering risks data loss on crash, mitigated with WAL (write-ahead log) enabled. In Zalo, this ensured continuous DS feature serving without interruptions.
- **Question**: Discuss trade-offs between ClickHouse and PostgreSQL in your architectures.
	- **Expected Answer (Expanded)**: ClickHouse excels in OLAP: Columnar storage for fast scans/aggregations on large Ads datasets (e.g., 2 TB/day logs), but lacks full ACID (only atomic inserts). PostgreSQL for OLTP: ACID transactions for metadata (e.g., campaign configs in Bronze layer), with row-level locking. In Zalo, used ClickHouse for BI/ML queries (2–10× faster, 30% cheaper storage) but PostgreSQL for transactional Silver/Gold layers needing updates (e.g., UPDATE ad_status). Trade-offs: ClickHouse scales horizontally easier (add shards) but queries are approximate (no full joins); Postgres better for complex joins but slower at scale without extensions like TimescaleDB. Hybrid: Airflow DAGs ingested to Postgres first for validation, then to ClickHouse. Chose based on workload—OLAP-heavy for DS training pipelines.
- **Question**: How did you implement projections in ClickHouse, and what was the impact?
	- **Expected Answer (Expanded)**: Projections store alternate sort orders/materializations: e.g., ALTER TABLE ad_logs ADD PROJECTION proj_campaign (SELECT * ORDER BY campaign_id, timestamp); This pre-sorts data blocks for queries filtering on campaign_id without rescanning. Populated via OPTIMIZE TABLE ad_logs FINAL or incrementally. In Ads, it sped up DA queries like SELECT sum(clicks) GROUP BY campaign_id from hours to minutes (5–10×), as it skipped irrelevant blocks. Impact: Reduced CPU/I/O by 40%, but added 10–15% storage. Trade-off: Projections rebuild on schema changes, so I limited to 2–3 per table; monitored with system.projections. Tied to BI optimizations, ensuring Metabase dashboards loaded instantly for biz teams.
- **Question**: Describe vacuuming and maintenance in your PostgreSQL Gold layer. 
	- **Expected Answer (Expanded)**: AUTOVACUUM tuned aggressively: ALTER TABLE gold_aggregates SET (autovacuum_vacuum_scale_factor = 0.05, autovacuum_analyze_scale_factor = 0.02) to trigger on 5% changes. Manual VACUUM ANALYZE during off-peak via cron/Airflow. For partitioned tables (e.g., by month: CREATE TABLE gold_aggregates PARTITION BY RANGE (timestamp)), detached old partitions for archiving. In TMA's unified Data Lake, this prevented bloat from frequent ELT updates, maintaining 3x query speeds. Impact: Reduced table size by 20% post-vacuum, fixed query plans with fresh stats. Trade-off: Vacuum can lock during full mode, so used VACUUM FREEZE for long-running; monitored pg_stat_all_tables for last_vacuum. Ensured no downtime for DA/BIZ access.
- **Question**: How did you query optimize for BI in ClickHouse (e.g., skip indexes)? 
	- **Expected Answer (Expanded)**: Skip indexes avoid reading unnecessary data blocks: e.g., ALTER TABLE ad_logs ADD INDEX idx_user_skip (user_id) TYPE minmax GRANULARITY 3; For blooms: TYPE bloom_filter(0.01) on strings. This skipped blocks where min/max or bloom didn't match filters, e.g., SELECT * WHERE user_id = 12345 scanned only relevant granules. In Zalo BI, optimized for Metabase queries on ad performance, reducing I/O by 50–70% and achieving 2–10× speedups. Trade-off: Indexes add insert overhead (+10ms) and storage (5%); I chose granularity=3 for balance (block size ~24k rows). Monitored effectiveness with EXPLAIN or system.query_log. For DS, combined with materialized views for feature queries.
- **Question**: What sharding/replication did you set up in ClickHouse for scalability? 
	- **Expected Answer (Expanded)**: Sharding via Distributed engine: CREATE TABLE ad_logs_dist ENGINE = Distributed('cluster_name', 'default', 'ad_logs', rand()); With 3 shards, data hashed by sharding_key (e.g., cityHash64(user_id)). Replication: ReplicatedMergeTree with ZooKeeper quorum (quorum=2). In Zalo's high-volume Ads (2 TB/day), this scaled reads/writes linearly—e.g., queries distributed across nodes, sustaining thousands/sec inserts. Failover: Automatic via replicas. Trade-off: Sharding complicates joins (use local tables + FINAL), and ZooKeeper is a SPOF, so used 3-node ZK ensemble. Monitored with clickhouse-keeper logs and system.clusters. For ML serving, ensured low-latency by colocating shards with compute nodes, reducing costs by 30% through efficient resource use.

# Topic 4: MLOps and Observability (Feature Drift, WanDB)
- **Question**: Explain your inline observability checks for training-serving skew. How did they work? 
	- **Expected Answer (Expanded)**: Inline checks were embedded in the ingestion pipeline to compare feature distributions between training data (historical batches) and serving data (real-time inferences). In Zalo Ads, where DS models predict click-through rates based on user features (e.g., age, interests), skew could cause accuracy drops. Implementation: In PySpark transformations within Airflow DAGs, I sampled ~1% of incoming data and computed stats (mean, stddev, histograms) via df.describe() or custom UDFs, then compared to stored training baselines using Kolmogorov-Smirnov (KS) test from scipy.stats.ks_2samp. If skew score >0.1, flagged via logging to Prometheus. Example code: from scipy import stats; skew = stats.ks_2samp(serving_sample['feature'], training_baseline['feature']).statistic; if skew > threshold: alert_slack('Skew detected in user_age'). This ran in parallel with main ETL, adding <5% overhead. Trade-off: Sampling reduces accuracy but keeps latency low; reduced MTTD by 60% by catching issues before DS retraining, preventing model degradation in production ads serving.
- **Question**: How did you use WanDB for tracking 100+ LLM experiments? What artifacts did you log? 
	- **Expected Answer (Expanded)**: At TMA, for LLM fine-tuning research (LoRA/QLoRA on models like GPT-2 for text generation tasks), I integrated WanDB as the central tracker: import wandb; wandb.init(project='llm-finetune', config={'lr':1e-4, 'epochs':5}). Logged metrics every epoch (wandb.log({'loss':train_loss, 'val_acc':0.85})), hyperparameters (via config), and artifacts: wandb.log_artifact('model_checkpoint', type='model', path='./model.pt') for versions, datasets (wandb.log_artifact('finetune_data', type='dataset', path='data.parquet')), and even plots (wandb.log({'confusion_matrix': wandb.plot.confusion_matrix(...)})). For 100+ experiments, used sweeps: wandb.sweep(sweep_config) to automate hyperparam search (e.g., grid on rank=[4,8,16] for LoRA). This ensured DS teams could compare runs via WanDB UI tables. Trade-off: API calls add minor latency (~100ms/log), but enabled full reproducibility; reduced experiment redo time by 50% through searchable logs.
- **Question**: Describe detecting feature drift in your ingestion pipeline. What tools/libraries? 
	- **Expected Answer (Expanded)**: Feature drift detection monitored shifts in data distributions over time, critical in Zalo Ads for features like user engagement scores drifting due to seasonal campaigns. Integrated into Kafka/Scala pipeline: Used Evidently library (Python/Scala wrappers) for PSI (Population Stability Index) or JS divergence. In code: Sampled stream batches via Kafka consumer, computed Evidently profiles (e.g., from evidently import ColumnMapping; report = calculate_data_drift(current_data, reference_data, column_mapping)), then if drift_score >0.2, pushed to alert topic. Libraries: Evidently for reports, SciPy/StatsModels for stats tests, integrated with Spark for scale. Ran every 10 min on windowed aggregates. Trade-off: Real-time detection increases compute (tuned to 1% sample), but prevented undetected degradation; combined with Airflow for batch validation, achieving 60% MTTD reduction for DS model inputs.
- **Question**: How did you ensure reproducibility in LLM fine-tuning with WanDB? 
	- **Expected Answer (Expanded)**: Reproducibility was key for TMA's 100+ experiments to allow DS to rerun or audit: In scripts, set seeds (torch.manual_seed(42), np.random.seed(42)), versioned code via Git commit hash logged as wandb.config['git_commit'], and artifacts for everything mutable—e.g., wandb.log_artifact('tokenizer', path='tokenizer.json') and full config YAML (wandb.save('config.yaml')). For LoRA: Used PEFT library with fixed adapters. To reproduce: wandb.init(run_id='existing_run'), wandb.restore('model.pt'). This created a "run resume" workflow. Trade-off: Artifact storage costs (used compression), but ensured exact recreations; in practice, allowed quick iterations on QLoRA ranks without data mismatches, saving DA teams time on result validation.
- **Question**: What alerts did you set for model degradation prevention? 
	- **Expected Answer (Expanded)**: Alerts were proactive: In Zalo, for DS ad models, set thresholds on drift/skew scores (e.g., PSI>0.3) via Prometheus rules (e.g., alert: FeatureDriftHigh expr: drift_metric > 0.3 for: 5m). Integrated with PagerDuty/Slack: Custom webhook from pipeline (e.g., in Scala: HttpClient.post('slack_url', jsonAlert)). Also, model-specific: If serving latency >200ms or accuracy drop >5% (from baseline logged in WanDB), trigger. For TMA LLMs, alerts on eval_loss increase during sweeps. This prevented degradation by auto-pausing deploys or notifying DS for retrain. Trade-off: Too sensitive leads to alert fatigue (tuned with historical data); reduced undetected issues by 60% MTTD, ensuring stable biz dashboards.
- **Question**: Discuss integrating observability with Kafka streams for real-time drift. 
	- **Expected Answer (Expanded)**: Integration sampled Kafka streams for low-latency checks: In Scala pipeline at Zalo, added a side consumer group (KafkaConsumer with group.id='observability') pulling 0.1–1% of messages via random offset skips. Computed rolling stats (e.g., using Apache Commons Math for online variance) and drift via Evidently in a Spark micro-batch. If detected, produced to alert topic. Code: val stream = KafkaUtils.createDirectStream(...); stream.mapPartitions { records => computeDrift(records.sample(0.01)) }. For Ads features (e.g., click rates drifting intraday), this enabled <10-min detection. Trade-off: Sampling adds minor throughput hit (<1%), but scalable with more consumers; no full integration needed ZooKeeper offsets for state. Reduced MTTD by 60%, preventing real-time model fails.
- **Question**: How did you visualize experiments in WanDB for team collaboration? 
	- **Expected Answer (Expanded)**: Visualizations fostered DS/DA collaboration at TMA: Logged plots like wandb.plot.line_series(xs=epochs, ys=[train_loss, val_loss], keys=['Train', 'Val'], title='Loss Curve') or wandb.plot.roc(y_true, y_pred) for classification. Custom dashboards: Grouped runs by tags (e.g., 'LoRA' vs 'QLoRA'), with parallel coordinates for hyperparams (wandb.plot.parallel_coords(table)). Shared via WanDB Reports: wandb.Report with embedded charts and notes. For 100+ experiments, this allowed filtering by metrics (e.g., best acc>0.9). In Ads context, similar for pipeline metrics. Trade-off: High-res plots increase storage, so downsampled data; enabled quick reviews, cutting meeting time by 30%.
- **Question**: What challenges in drift detection for high-dimensional features (e.g., embeddings from LLMs), and how did you address them? 
	- **Expected Answer (Expanded)**: Challenges: High-dim (e.g., 768+ for LLM embeddings) makes KS/PSI computationally expensive (O(n^2)) and noisy. In TMA LLMs or Zalo user embeddings, curse of dimensionality hides subtle drifts. Addressed: Dimensionality reduction first (PCA via sklearn.decomposition.PCA(n_components=50) or UMAP), then drift tests on reduced space. For distance: Cosine similarity distributions or MMD (from alibi-detect). Sampled 10k–50k rows to fit in memory. Dynamic thresholds via historical variance (e.g., mean + 3*std). Reduced false positives by 40%. Trade-off: Reduction loses info, but validated with spot checks; ensured scalable for DS serving without cluster overload.
- **Question**: How did you integrate drift detection into your existing Airflow DAGs without significantly increasing runtime? 
	- **Expected Answer (Expanded)**: Integration as lightweight tasks: In Zalo Airflow DAGs, added PythonOperator post-ingestion (e.g., @task def check_drift(): sample = df.sample(0.01); drift = evidently_drift(sample, baseline); if drift: alert()). Ran on subsampled data via Spark sample(fraction=0.01), parallelized. For minimal runtime: Asynchronous via concurrent.futures, or offloaded to side DAG triggered conditionally. Added <5–10% overhead (e.g., 2-min DAG → 2:10). Trade-off: Subsampling misses rare drifts, mitigated by cumulative checks; achieved 60% MTTD reduction without SLA breaches for DS/BIZ pipelines.
- **Question**: Describe a specific case where your observability checks prevented model degradation. What was the root cause and resolution? 
	- **Expected Answer (Expanded)**: In Zalo Ads, checks caught skew in 'user_session_duration' feature: Inline KS test flagged p-value<0.05 during peak traffic. Root cause: Upstream log change (new app version altered timestamp formats), skewing distributions and dropping model acc by ~10% in simulations. Alerted DS within hours (60% faster MTTD). Resolution: Fixed transformation in Scala pipeline (added format coercion), retrained model on corrected data, rolled back serving until verified. Prevented revenue loss from poor ad targeting. Trade-off: False alert risk, but tuned thresholds; highlighted need for schema enforcement upstream.

# Topic 5: Kubernetes and Container Orchestration (including Docker/HPC migrations)

1. **Question**: Walk through the migration process you did from Kubernetes to on-prem HPC for GPU workloads. What pain points drove the decision? 
	**Expected Answer**: Pain points: K8s GPU scheduling overhead, pod eviction during node maintenance, inconsistent GPU utilization (~60–70%). Migration: Exported Docker images, rewrote job specs to Slurm sbatch scripts, used shared NFS → Lustre for faster checkpointing. Improved GPU util to 90%+, failure rate from 10% to ≤5%.
2. **Question**: How did you handle persistent storage and checkpointing in the HPC/Slurm environment after migration? 
	 **Expected Answer**: Used Slurm --gres=gpu:N, bind-mounted shared filesystem (Lustre) for model checkpoints. Implemented custom checkpoint callback in training code to save every N steps to unique path (job_id + epoch). Recovery: Slurm --dependency=afterok + requeue logic.
3. **Question**: What Kubernetes features/configs did you use before the migration for Spark or Airflow workloads? 
	 **Expected Answer**: Spark on K8s via spark-submit --master k8s://... with dynamic allocation. Airflow: KubernetesExecutor. Resource requests/limits for CPU/memory/GPU, pod anti-affinity for spreading, HorizontalPodAutoscaler for workers.
4. **Question**: Explain how you debugged a pod failure or OOM in a Kubernetes-based data job.
	**Expected Answer**: kubectl describe pod → events, kubectl logs, kubectl exec to inspect. Checked resource requests/limits mismatch, heap dumps if JVM (Spark), Prometheus metrics for node pressure. Common fix: Increased memory requests or tuned spark.executor.memoryOverhead.
5. **Question**: How would you deploy a stateful Kafka cluster on Kubernetes today? What operators or Helm charts? 
	**Expected Answer**: Strimzi Kafka Operator (preferred for production). Custom CRDs for Kafka/KafkaConnect/ZooKeeper. PersistentVolumeClaims with storageClass for data durability. Monitoring via Prometheus + Grafana dashboards from Strimzi.
6. **Question**: Describe resource management and quotas you applied in K8s for mixed workloads (Spark, Airflow, etc.). 
	**Expected Answer**: Namespaces with ResourceQuota (CPU, memory, pods). LimitRange for defaults. PriorityClass for critical pipelines. Node selectors/taints for GPU nodes.
7. **Question**: What was your approach to secret management in Kubernetes during your time at Zalo?
	**Expected Answer**: Sealed Secrets or external-secrets operator pulling from HashiCorp Vault / AWS SSM. No plaintext in manifests. Mounted as volumes or envFrom.
8. **Question**: How did you handle rolling updates or canary deployments for data services on K8s? 
	**Expected Answer**: Deployment with maxSurge=25%, maxUnavailable=0 for zero-downtime. Service with sessionAffinity if needed. For Spark jobs: Treated as Jobs/CronJobs with concurrencyPolicy=Replace.
9. **Question**: Discuss any Istio or service mesh usage you had (or would consider) for observability in data pipelines. 
	**Expected Answer**: (Likely none at Zalo, but reasoned) Would use for mTLS, traffic shifting, golden metrics (latency, error rate). Kiali + Jaeger for tracing Kafka → Spark calls.
10. **Question**: How do you compare Slurm job scheduling vs. Kubernetes for ML training workloads? Trade-offs from your experience. 
	**Expected Answer**: Slurm: Better raw GPU utilization, simpler for batch/HPC, lower overhead. K8s: Better for microservices, auto-scaling, unified platform. Trade-off: Slurm faster for large jobs, K8s more flexible for hybrid (CPU + GPU) clusters.

### Topic 6: Cloud Infrastructure and Data Lakes (AWS/GCP)

1. **Question**: Describe the architecture of the automated ELT pipeline you built with AWS Glue and PySpark. 
	**Expected Answer**: S3 as landing → Glue Crawler → Catalog → Glue Job (PySpark) for transformations → Parquet in refined zone. Triggered by S3 events or Airflow. Reduced manual prep from days to hours.
2. **Question**: How did you implement the Bronze/Silver/Gold layers in your AWS data lake? **Expected Answer**: Bronze: raw JSON/CSV as-is. Silver: cleaned, deduped, typed Parquet. Gold: aggregated, business-ready. Delta Lake or Iceberg for ACID if needed (though not mentioned, likely Parquet + partitioning).
3. **Question**: What partitioning and file format strategies did you use in Glue/S3 to achieve good performance? 
	**Expected Answer**: Partition by date/source, Parquet with Snappy compression. Hive-style partitioning. Glue bookmarks for incremental processing.
4. **Question**: How did you handle schema evolution in your AWS Glue ETL jobs? 
	**Expected Answer**: Glue schema inference + manual overrides in catalog. Backward-compatible changes (add columns nullable). Used dynamic frames for flexible schema.
5. **Question**: Compare BigQuery and ClickHouse based on your experience. When would you choose one over the other? 
	**Expected Answer**: BigQuery: serverless, great for ad-hoc BI, columnar but expensive at scale. ClickHouse: self-managed, cheaper storage, faster for high-ingestion OLAP. Chose ClickHouse for cost (30% lower) and write-heavy ML feature serving.
6. **Question**: Describe any cost optimization techniques you applied in AWS Glue or S3.
	**Expected Answer**: S3 Intelligent-Tiering / Glacier for cold data. Glue job bookmarks + incremental loads. Right-sized DPUs. Reduced costs via partitioning and compression.
7. **Question**: How would you ingest real-time data into a cloud data lake (e.g., from Kafka)?
	**Expected Answer**: Kafka Connect → S3 Sink (parquet) or MSK + Glue streaming ETL. Hudi/Delta for upserts if needed.
8. **Question**: What monitoring did you set up for Glue jobs and S3 data pipelines?
	**Expected Answer**: CloudWatch metrics (job duration, errors), Glue job logs to CloudWatch Logs. Alerts on failed jobs or high DPU usage.
9. **Question**: Did you use AWS Lake Formation or similar for governance? If not, how did you handle access control? **Expected Answer**: (Likely basic) IAM roles for Glue service, S3 bucket policies. Column-level security via Glue Data Catalog if advanced.
10. **Question**: How would you design a multi-cloud or hybrid cloud data lake strategy based on your GCP + AWS experience? **Expected Answer**: Standardize on open formats (Parquet, Delta), use MinIO/S3-compatible for abstraction. Airflow or Dagster for orchestration across clouds. Avoid lock-in on proprietary features.

### Topic 7: CI/CD Pipelines and Automation (GitLab/GitHub Actions)

1. **Question**: Describe your CI/CD setup for deploying Airflow DAGs and data pipeline code at Zalo. 
	**Expected Answer**: GitLab repo → .gitlab-ci.yml with stages: lint, test, deploy. DAG validation, unit tests (pytest), then rsync or kubectl apply to Airflow namespace.
2. **Question**: How did you ensure zero-downtime or safe deployment of DAG changes? **Expected Answer**: New DAG version with different ID or tag. Parallel run old/new, switch traffic via Airflow variables or tags. Pause old DAG after validation.
3. **Question**: What automated tests did you run in CI for data pipelines? 
	**Expected Answer**: Unit tests for transformations (Great Expectations or custom), schema validation, backfill simulation on small data.
4. **Question**: How did you handle secrets in GitLab CI/CD pipelines? 
	**Expected Answer**: GitLab CI/CD variables (masked/protected), injected as env vars. No commits of creds.
5. **Question**: Describe a blue-green or canary deployment pattern you used for APIs or data services. 
	**Expected Answer**: For REST APIs: Two K8s deployments, Istio or ingress traffic split. For data: Shadow mode (run new pipeline in parallel, compare outputs).
6. **Question**: How did you automate Docker image builds and pushes in your pipeline? **Expected Answer**: GitLab runner with Docker-in-Docker, build → tag with commit SHA → push to registry. Multi-stage builds for smaller images.
7. **Question**: What quality gates did you enforce before production deployment? **Expected Answer**: Code coverage >80%, no lint errors (black, flake8), DAG parse success, manual approval for prod.
8. **Question**: How did you rollback a bad deployment in your CI/CD flow? **Expected Answer**: Git tag rollback, redeploy previous image. For DAGs: Revert commit or disable new DAG.
9. **Question**: Did you use infrastructure-as-code (Terraform) for any cloud resources? If not, why? **Expected Answer**: (Likely not primary) Focused on app-level CI/CD. If used, Terraform for Glue jobs/catalog, but manual for some.
10. **Question**: How would you improve your current CI/CD for faster feedback loops in a robotics data platform team? **Expected Answer**: Add preview environments (namespace per MR), faster unit tests, parallel stages, caching Docker layers, GitLab Auto DevOps features.
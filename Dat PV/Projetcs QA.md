# Topic 1: Data Pipeline Orchestration with Apache Airflow
- **Question**: Walk through the structure of one of your complex Airflow DAGs. How did you incorporate PySpark tasks? 
	- **Expected Answer (Expanded)**: A complex DAG might start with a sensor task (e.g., ExternalTaskSensor or FileSensor) to wait for upstream data arrival, like ad log files in S3 or Kafka topics signaling completion. Then, a PythonOperator for lightweight preprocessing (e.g., validating file schemas), followed by a SparkSubmitOperator for heavy PySpark transformations—submitting a .py script with args like --master yarn --executor-memory 4g. Dependencies are chained with >> or BitShiftOperator for parallelism (e.g., multiple branches for different ad campaigns). Finally, a BashOperator or PythonOperator for loading to ClickHouse and notifying Metabase. In my Zalo Ads work, a DAG processed daily ad impressions: Ingestion >> PySpark aggregation (e.g., groupBy campaign_id, sum(clicks)) >> Load, with XCom pulls for dynamic params like date ranges. Idempotency via task_instance.xcom_pull to check prior runs, ensuring reruns don't duplicate data for DS model training.
- **Question**: How did you achieve 95%+ SLA compliance in your Airflow setups? What monitoring did you implement? 
	- **Expected Answer (Expanded)**: SLAs were enforced using Airflow's built-in SLA mechanism—defining sla=timedelta(hours=2) per task and sla_miss_callback to trigger alerts (e.g., Slack webhook with details like task_id, execution_date). Retries were key: default_retries=3 with exponential retry_delay (e.g., timedelta(minutes=5 * (2 ** retry_number))). For Ads pipelines, where delays could impact real-time bidding DS models, I added custom health checks via PythonOperator running SQL queries on ClickHouse to verify data freshness post-load. Monitoring: Airflow's Flower UI for task queues, plus StatsD exporter pushing metrics (e.g., task_duration, queue_length) to Prometheus/Grafana dashboards. Alerts on >90% CPU or task failures >5%. This combo kept SLAs at 95%+, reducing biz team complaints about stale Metabase reports from weekly to near-zero.
- **Question**: Explain how you handled dynamic DAGs or parameterized workflows in Airflow for varying data volumes. 
	- **Expected Answer (Expanded)**: For dynamic behavior, I used Jinja templating with macros like {{ ds }} for dates or {{ params.volume_threshold }} pulled from DAG config. BranchPythonOperator decided paths: e.g., if data volume >1TB (checked via PySpark count()), branch to high-resource Spark job; else, lightweight Python task. In Ads at Zalo, pipelines varied by campaign scale—parameterized via JSON configs in Variables or Connections, allowing DS teams to override via Airflow API triggers (e.g., airflow dags trigger --conf '{"campaign_id":123}'). For TMA test scripts, I used SubDAGOperator for reusable modules like data validation. Trade-off: Dynamics add flexibility but complexity; I mitigated with DAG linting in CI to catch template errors early.
- **Question**: What challenges arose with task dependencies in 50+ DAGs, and how did you optimize them? 
	- **Expected Answer (Expanded)**: With 50+ DAGs, cross-DAG dependencies caused bottlenecks—e.g., one DAG waiting on another's completion led to cascading delays in Ads reporting. Used ExternalTaskSensor with external_dag_id/task_id to sync, but optimized by grouping into meta-DAGs with TriggerDagRunOperator for orchestration. Fan-out/in: DummyOperator as join points. For optimization, pools limited concurrent Spark tasks (e.g., pool=spark_pool, slots=10) to avoid cluster overload. Priority_weight tuned for biz-critical paths (e.g., DA dashboards > experimental DS jobs). In practice, this cut average DAG runtime by 20% during peak ad traffic, preventing scheduler overload (monitored via scheduler_heartbeat_sec=5).
- **Question**: Describe integrating Airflow with Kafka for triggering streaming-related tasks.
	- **Expected Answer (Expanded)**: Integration via KafkaSensor (from airflow.providers.apache.kafka) polling a topic for messages (e.g., ad event completion signals). On poke success, it pushes XCom (e.g., message payload as params) to downstream tasks like SparkStreamingOperator. For at-least-once semantics, committed offsets manually in a callback. In Zalo Ads, this triggered near-real-time aggregations: Kafka topic 'ad_events' → Sensor → PySpark job aggregating clicks for DS serving. If no built-in sensor, custom PythonOperator with kafka-python client. Trade-off: Polling adds latency; for high-volume, used webhook from Kafka consumer to Airflow API (trigger_dag). Ensured no duplicate triggers via unique message IDs in XCom.
- **Question**: How did you manage secrets and configurations in your Airflow DAGs securely?
	- **Expected Answer (Expanded)**: Secrets via Airflow Connections (e.g., conn_id='clickhouse_prod' with encrypted password using Fernet key). In code: from airflow.hooks.base import BaseHook; hook = BaseHook.get_connection('clickhouse_prod'). Variables for non-secrets like API endpoints, stored in Metadata DB or AWS SSM backend for encryption. No hardcoding—always hook.get_extra() or Variable.get('key'). In Ads team, this secured DS access to sensitive bid data; audited via Airflow logs. For TMA projects, used environment variables injected via Kubernetes secrets. Best practice: Rotate Fernet keys periodically, and use RBAC to limit operator access.
- **Question**: Discuss scaling Airflow for high-throughput (e.g., for 2 TB/day integrations). What executor did you use?
	- **Expected Answer (Expanded)**: For scale, switched to CeleryExecutor with Redis backend (broker_url=redis://host:6379/0) for distributed task queuing, supporting 100+ workers. KubernetesExecutor for dynamic pods in cloud. Tuned scheduler (scheduler_heartbeat_sec=1, parallelism=128) and database (PostgreSQL with vacuuming). In Zalo's 2 TB/day logs, this handled 50+ DAGs by auto-scaling workers via K8s HPA on CPU>70%. Trade-off: Celery adds Redis dependency but enables horizontal scale; LocalExecutor suffices for small setups but bottlenecks at high throughput. Monitored with Prometheus for worker health, preventing OOM by resource limits.
- **Question**: How did you implement error notification and recovery in your DAGs? 
	- **Expected Answer (Expanded)**: on_failure_callback per task/DAG to call a function sending Slack/Email with context (e.g., task_instance, exception). Recovery: retry_exponential_backoff=True, with max_retries=5. For non-retriable, marked failed and triggered manual rerun via CLI (airflow tasks clear). In Ads, added custom rollback: e.g., PythonOperator to delete partial ClickHouse inserts on failure. Catchup=False for streaming DAGs to avoid backlog. Example: If PySpark OOM, callback logs spark.driver.log and alerts DS team. Reduced downtime for biz dashboards from hours to minutes.
- **Question**: What testing strategies did you use for Airflow DAGs before production?
	- **Expected Answer (Expanded)**: Unit tests with pytest-airflow: Mock tasks (e.g., patch 'airflow.operators.spark.SparkSubmitOperator') and assert dependencies. Integration: LocalExecutor in Docker compose, running 'airflow dags test my_dag' with sample data. For Ads pipelines, Great Expectations in a task for data validation. CI via GitLab: Stages for lint (airflow-dag-lint), unit, and smoke tests (backfill on historical dates). In TMA test script project, added end-to-end tests simulating failures. Coverage >80%; this caught issues like missing imports before deploy, ensuring 95% SLA.
- **Question**: How would you version-control and deploy updates to your 50+ DAGs without downtime? 
	- **Expected Answer (Expanded)**: Git for DAG files in a monorepo, with branches for features. Deploy: CI pipeline syncs to Airflow server via rsync or git pull in a webhook. For zero-downtime, new DAGs get versioned IDs (e.g., ads_pipeline_v2), run in parallel with v1 until validation (compare outputs via sensor). Pause/resume old DAG post-switch. In K8s, rolling updates with new image tags. In Zalo, this allowed hotfixes for DS pipelines without interrupting biz reports—e.g., add a task mid-DAG by redeploying and clearing downstream. Trade-off: Versioning bloats scheduler, so prune old DAGs via airflow dags delete.


# Topic 2: Streaming and Near-Real-Time Data Processing (Kafka/Spark/Scala)
- **Question**: Describe the architecture of your 2 TB/day centralized logs pipeline in Scala. What role did Kafka play, and how did you integrate it with Spark Streaming? 
	- **Expected Answer**: Kafka as the ingestion layer for durable, partitioned message queuing (e.g., topics partitioned by log source or timestamp for parallelism). Spark Streaming (or Structured Streaming) for processing: Micro-batches or continuous mode for transformations like filtering, aggregating (e.g., windowed counts on log events). Integration via Kafka connector (e.g., spark-sql-kafka-0-10), with consumer groups for offset management. He should mention Scala traits/objects for modular code, and how parallelism reduced latency (e.g., executor cores matching partitions).
- **Question**: How did you achieve parallel transformations in your Scala pipeline? Provide a code snippet example and explain the performance impact. 
	- **Expected Answer**: Used Spark's RDD/DataFrame APIs in Scala for mapPartitions or foreachPartition on streams. Example: stream.mapPartitions { iter => iter.map(transformLog) } with Scala futures for intra-partition concurrency. Impact: Reduced latency by distributing compute across clusters (e.g., from 1h to 5-10 min via 10-20x parallelism), mentioning shuffle costs and how he tuned spark.default.parallelism.
- **Question**: What challenges did you face with data skew in Kafka topics, and how did you mitigate them in your pipeline? 
	- **Expected Answer**: Skew from uneven log volumes per source leading to hot partitions. Mitigation: Custom partitioner in Kafka producer (Scala class extending Partitioner, hashing on balanced keys like source+timestamp). In Spark, repartition() or coalesce() post-ingest. He should quantify: e.g., reduced max task time from 30 min to even distribution, preventing OOM.
- **Question**: Explain how you ensured exactly-once semantics in your Kafka-Spark streaming setup. What configurations were key? 
	- **Expected Answer**: Enabled idempotent producers in Kafka (enable.idempotence=true), combined with Spark's output commit protocol in Structured Streaming (e.g., foreachBatch with transactional sinks). Offsets committed atomically via Kafka's transactional API. Key configs: acks=all, retries high, spark.streaming.kafka.enableExactlyOnceSemantics. He should note trade-offs: slight throughput hit but prevented duplicates in aggregations.
- **Question**: In Scala, how did you handle schema evolution for streaming logs in your pipeline? Give an example of Avro or JSON handling. 
	- **Expected Answer**: Used Schema Registry with Kafka (Confluent or similar) for Avro-serialized messages. In Scala, case classes for schemas, with Spark's from_avro/to_avro functions. Example: val schema = SchemaRegistryClient.getLatest("logs-schema"); evolve by backward-compatible fields. Handled via try-catch for mismatches, logging drifts. Ensured no downtime during schema changes.
- **Question**: Describe fault tolerance mechanisms in your Kafka pipeline. How did you recover from broker failures? 
	- **Expected Answer**: Kafka replication factor (e.g., 3) for high availability, ISR for in-sync replicas. In Spark, checkpointing streams to HDFS/S3 (spark.streaming.checkpoint.dir). Recovery: Auto-offset reset to earliest/latest, with Spark's WAL for exactly-once. He should explain leader election process and how it minimized his pipeline's downtime (e.g., <1 min recovery).
- **Question**: How did you monitor and tune latency in your streaming pipeline? What metrics and tools did you use? 
	- **Expected Answer**: Monitored via Kafka JMX (e.g., under-replicated partitions, request latency) and Spark UI (processing time per batch). Tools: Prometheus/Grafana for custom Scala metrics (e.g., using Micrometer). Tuning: Increased partitions, tuned batch.interval.ms (e.g., from 1s to 500ms), added more consumers. Result: 5-10 min end-to-end, with alerts on >90th percentile latency.
- **Question**: What role did Scala play in your pipeline's efficiency compared to Python? Provide a specific example from your work. 
	- **Expected Answer**: Scala's type safety and performance (JVM optimizations) over Python's dynamism. Example: Immutable collections (Seq/Vector) for thread-safe transformations vs. Python lists; used Akka actors for async processing if applicable. Efficiency: Faster compile-time checks reduced bugs, and JVM garbage collection tuning cut GC pauses in long-running streams.
- **Question**: How would you extend your pipeline to handle backpressure in high-volume scenarios? Explain with Kafka/Spark configs. 
	- **Expected Answer**: Kafka: max.in.flight.requests.per.connection=1 for order preservation. Spark: Backpressure enabled (spark.streaming.backpressure.enabled=true), dynamic allocation for executors. Rate limiting via spark.streaming.kafka.maxRatePerPartition. He should describe a scenario: e.g., during log spikes, throttled to prevent OOM, maintaining 5-10 min latency.
- **Question**: Discuss error handling in your streaming transformations. How did you manage retries and dead-letter queues? 
	- **Expected Answer**: In Scala, Try/Success/Failure pattern for transformations, with retries via exponential backoff (e.g., using Scala's Future.retry). Errors routed to DLQ Kafka topic (separate producer). Spark's foreachPartition for batch commits. Ensured idempotency to avoid duplicates on retries, reducing failure rates as in his HPC migration (from 10% to 5%).